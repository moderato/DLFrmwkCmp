{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Comparison of Deep Learning Frameworks\n",
    "\n",
    "## Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn import model_selection as ms\n",
    "from sys import platform\n",
    "from prp_img import getImageSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform == \"darwin\":\n",
    "    root = \"/Users/moderato/Downloads/GTSRB/try\"\n",
    "else:\n",
    "    root = \"/home/zhongyilin/Desktop/GTSRB/try\"\n",
    "print(root)\n",
    "resize_size = (49, 49)\n",
    "epoch_num = 60\n",
    "trainImages, trainLabels, testImages, testLabels = getImageSets(root, resize_size)\n",
    "x_train, x_valid, y_train, y_valid = ms.train_test_split(trainImages, trainLabels, test_size=0.2, random_state=542)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intel Nervana Neon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neon.backends import gen_backend, cleanup_backend\n",
    "from neon.initializers import Gaussian, Constant, GlorotUniform\n",
    "from neon.layers import GeneralizedCost, Affine\n",
    "from neon.layers import Conv as neon_Conv, Dropout as neon_Dropout, Pooling as neon_Pooling\n",
    "from neon.transforms import Rectlin, Softmax, CrossEntropyMulti, Misclassification, TopKMisclassification\n",
    "from neon.models import Model\n",
    "from neon.optimizers import GradientDescentMomentum as neon_SGD, RMSProp as neon_RMSProp, ExpSchedule\n",
    "from neon.callbacks.callbacks import Callbacks, Callback, LossCallback\n",
    "from neon.data.dataiterator import ArrayIterator\n",
    "from timeit import default_timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SelfCallback(LossCallback):\n",
    "    def __init__(self, eval_set, epoch_freq):\n",
    "        super(SelfCallback, self).__init__(eval_set=eval_set, epoch_freq=epoch_freq)\n",
    "        self.train_batch_time = None\n",
    "        self.total_batch_index = 0\n",
    "        \n",
    "    def on_train_begin(self, callback_data, model, epochs):\n",
    "        super(SelfCallback, self).on_train_begin(callback_data, model, epochs)\n",
    "        \n",
    "        # Save training time per batch\n",
    "        points = callback_data['config'].attrs['total_minibatches']\n",
    "        tb = callback_data.create_dataset(\"time/train_batch\", (points,))\n",
    "        tb.attrs['time_markers'] = 'minibatch'\n",
    "        \n",
    "    def on_minibatch_begin(self, callback_data, model, epoch, minibatch):\n",
    "        self.train_batch_time = default_timer()\n",
    "\n",
    "    def on_minibatch_end(self, callback_data, model, epoch, minibatch):\n",
    "        callback_data[\"time/train_batch\"][self.total_batch_index] = (default_timer() - self.train_batch_time)\n",
    "        self.total_batch_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = None\n",
    "epoch_num = 50\n",
    "\n",
    "neon_backends = [\"cpu\", \"mkl\", \"gpu\"]\n",
    "neon_gaussInit = Gaussian(loc=0.0, scale=0.01)\n",
    "d = dict()\n",
    "neon_lr = {\"cpu\": 0.01, \"mkl\": 0.0005, \"gpu\": 0.01}\n",
    "run_or_not = {\"cpu\": False, \"mkl\": True, \"gpu\": False}\n",
    "\n",
    "cleanup_backend()\n",
    "\n",
    "for b in neon_backends:\n",
    "    if run_or_not[b]:\n",
    "        print(\"Use {} as backend.\".format(b))\n",
    "\n",
    "        # Set up backend\n",
    "        # backend: 'cpu' for single, 'mkl' for multi-thread cpu, and 'gpu' for gpu\n",
    "        be = gen_backend(backend=b, batch_size=batch_size, rng_seed=542, datatype=np.float32)\n",
    "        print(type(be))\n",
    "\n",
    "        # Make iterators\n",
    "        x_train, x_valid, neon_y_train, neon_y_valid = ms.train_test_split(trainImages, trainLabels, test_size=0.2, random_state=542)\n",
    "        neon_train_set = ArrayIterator(X=np.asarray([t.flatten().astype('float32')/255 for t in x_train]), y=np.asarray(neon_y_train), make_onehot=True, nclass=43, lshape=(3, resize_size[0], resize_size[1]))\n",
    "        neon_valid_set = ArrayIterator(X=np.asarray([t.flatten().astype('float32')/255 for t in x_valid]), y=np.asarray(neon_y_valid), make_onehot=True, nclass=43, lshape=(3, resize_size[0], resize_size[1]))\n",
    "        neon_test_set = ArrayIterator(X=np.asarray([t.flatten().astype('float32')/255 for t in testImages]), y=np.asarray(testLabels), make_onehot=True, nclass=43, lshape=(3, resize_size[0], resize_size[1]))\n",
    "\n",
    "        # Construct CNN\n",
    "        layers = []\n",
    "        layers.append(neon_Conv((5, 5, 64), strides=2, init=neon_gaussInit, bias=Constant(0.0), activation=Rectlin(), name=\"neon_conv1\"))\n",
    "        layers.append(neon_Pooling(2, op=\"max\", strides=2, name=\"neon_pool1\"))\n",
    "        layers.append(neon_Conv((3, 3, 512), strides=1, padding=1, init=neon_gaussInit, bias=Constant(0.0), activation=Rectlin(), name=\"neon_conv2\"))\n",
    "        layers.append(neon_Pooling(2, op=\"max\", strides=2, name=\"neon_pool2\"))\n",
    "    #     layers.append(neon_Pooling(5, op=\"avg\", name=\"neon_global_pool\"))\n",
    "        layers.append(Affine(nout=2048, init=neon_gaussInit, bias=Constant(0.0), activation=Rectlin(), name=\"neon_fc1\"))\n",
    "        layers.append(neon_Dropout(keep=0.5, name=\"neon_drop_out\"))\n",
    "        layers.append(Affine(nout=43, init=neon_gaussInit, bias=Constant(0.0), activation=Softmax(), name=\"neon_fc2\"))\n",
    "\n",
    "        # Initialize model object\n",
    "        mlp = Model(layers=layers)\n",
    "\n",
    "        # Costs\n",
    "        neon_cost = GeneralizedCost(costfunc=CrossEntropyMulti())\n",
    "\n",
    "        # Model summary\n",
    "        mlp.initialize(neon_train_set, neon_cost)\n",
    "        #     print(mlp)\n",
    "\n",
    "        # Learning rules\n",
    "\n",
    "        neon_optimizer = neon_SGD(neon_lr[b], momentum_coef=0.9, schedule=ExpSchedule(0.2))\n",
    "    #     neon_optimizer = neon_RMSProp(learning_rate=0.0001, decay_rate=0.95)\n",
    "\n",
    "        # Benchmark for 20 minibatches\n",
    "        d[b] = mlp.benchmark(neon_train_set, cost=neon_cost, optimizer=neon_optimizer)\n",
    "\n",
    "        # Reset model\n",
    "        mlp = None\n",
    "        mlp = Model(layers=layers)\n",
    "        mlp.initialize(neon_train_set, neon_cost)\n",
    "\n",
    "        # Callbacks: validate on validation set\n",
    "        callbacks = Callbacks(mlp, eval_set=neon_valid_set, metric=Misclassification(3), output_file=root+\"/callback_data_{}.h5\".format(b))\n",
    "        callbacks.add_callback(SelfCallback(eval_set=neon_valid_set, epoch_freq=1))\n",
    "\n",
    "        # Fit\n",
    "        start = time.time()\n",
    "        mlp.fit(neon_train_set, optimizer=neon_optimizer, num_epochs=epoch_num, cost=neon_cost, callbacks=callbacks)\n",
    "        print(\"Neon training finishes in {:.2f} seconds.\".format(time.time() - start))\n",
    "\n",
    "        # Result\n",
    "        results = mlp.get_outputs(neon_valid_set)\n",
    "\n",
    "        # Print error on validation set\n",
    "        start = time.time()\n",
    "        neon_error_mis = mlp.eval(neon_valid_set, metric=Misclassification())*100\n",
    "        print('Misclassification error = {:.1f}%. Finished in {:.2f} seconds.'.format(neon_error_mis[0], time.time() - start))\n",
    "\n",
    "        start = time.time()\n",
    "        neon_error_top3 = mlp.eval(neon_valid_set, metric=TopKMisclassification(3))*100\n",
    "        print('Top 3 Misclassification error = {:.1f}%. Finished in {:.2f} seconds.'.format(neon_error_top3[2], time.time() - start))\n",
    "\n",
    "        start = time.time()\n",
    "        neon_error_top5 = mlp.eval(neon_valid_set, metric=TopKMisclassification(5))*100\n",
    "        print('Top 5 Misclassification error = {:.1f}%. Finished in {:.2f} seconds.'.format(neon_error_top5[2], time.time() - start))\n",
    "\n",
    "        mlp.save_params(root + \"/saved_models/neon_weights_{}.prm\".format(b))\n",
    "\n",
    "        # Print error on test set\n",
    "        start = time.time()\n",
    "        neon_error_mis_t = mlp.eval(neon_test_set, metric=Misclassification())*100\n",
    "        print('Misclassification error = {:.1f}% on test set. Finished in {:.2f} seconds.'.format(neon_error_mis_t[0], time.time() - start))\n",
    "\n",
    "        start = time.time()\n",
    "        neon_error_top3_t = mlp.eval(neon_test_set, metric=TopKMisclassification(3))*100\n",
    "        print('Top 3 Misclassification error = {:.1f}% on test set. Finished in {:.2f} seconds.'.format(neon_error_top3_t[2], time.time() - start))\n",
    "\n",
    "        start = time.time()\n",
    "        neon_error_top5_t = mlp.eval(neon_test_set, metric=TopKMisclassification(5))*100\n",
    "        print('Top 5 Misclassification error = {:.1f}% on test set. Finished in {:.2f} seconds.'.format(neon_error_top5_t[2], time.time() - start))\n",
    "\n",
    "        cleanup_backend()\n",
    "        mlp = None\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Draw figures\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from neon.visualizations.data import h5_cost_data\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (10,8)\n",
    "\n",
    "train_cost_batch = pd.DataFrame()\n",
    "valid_cost_epoch = pd.DataFrame()\n",
    "train_epoch_mark = dict()\n",
    "# neon_backends = [\"mkl\", \"cpu\", \"gpu\"]\n",
    "\n",
    "fig1 = plt.figure()\n",
    "ax1 = fig1.add_subplot(1,1,1)\n",
    "fig2, ax2 = plt.subplots(1, len(neon_backends))\n",
    "markers = ['o', 'x', 'p']\n",
    "\n",
    "for i in range(len(neon_backends)):\n",
    "    b = neon_backends[i]\n",
    "    \n",
    "    f = h5py.File(root+\"/callback_data_{}.h5\".format(b), \"r\")\n",
    "    keys = list(f['.'].keys())\n",
    "    \n",
    "    train_cost_batch['neon_{}_loss'.format(b)] = pd.Series(f['.']['cost']['train'][()])\n",
    "    train_cost_batch['neon_{}_t'.format(b)] = pd.Series(f['.']['time']['train_batch'][()]).cumsum()\n",
    "    \n",
    "    valid_cost_epoch['neon_{}_loss'.format(b)] = pd.Series(f['.']['cost']['loss'][()])\n",
    "    valid_cost_epoch['neon_{}_t'.format(b)] = pd.Series(f['.']['time']['loss'][()])\n",
    "    \n",
    "    tmp = (f['.']['time_markers']['minibatch'][()]-1).astype(int).tolist()\n",
    "    tmp.pop()\n",
    "    tmp = [0] + tmp\n",
    "    train_epoch_mark['neon_{}_mark'.format(b)] = tmp\n",
    "    \n",
    "    ax1.plot(train_cost_batch['neon_{}_t'.format(b)].iloc[train_epoch_mark['neon_{}_mark'.format(b)]], \\\n",
    "             train_cost_batch['neon_{}_loss'.format(b)].iloc[train_epoch_mark['neon_{}_mark'.format(b)]], marker=markers[i])\n",
    "    \n",
    "    ax2[i].plot(range(len(train_epoch_mark['neon_{}_mark'.format(b)])), \\\n",
    "             train_cost_batch['neon_{}_loss'.format(b)].iloc[train_epoch_mark['neon_{}_mark'.format(b)]], marker=markers[0])\n",
    "    ax2[i].plot(range(len(train_epoch_mark['neon_{}_mark'.format(b)])), valid_cost_epoch['neon_{}_loss'.format(b)], marker=markers[1])\n",
    "    ax2[i].legend(loc='best')\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "ax1.legend(loc='best')\n",
    "plt.show()\n",
    "fig1.savefig(root+\"/pics/neon_train_loss_time.png\", dpi=fig1.dpi)\n",
    "fig2.savefig(root+\"/pics/neon_train_loss_epoch.png\", dpi=fig2.dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras with different multiple backends (Tensorflow, Theano, CNTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D as keras_Conv\n",
    "from keras.layers import MaxPooling2D as keras_MaxPooling, GlobalAveragePooling2D as keras_AveragePooling\n",
    "from keras.layers import Dropout as keras_Dropout, Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import image\n",
    "from keras.initializers import RandomNormal, Constant as keras_Constant\n",
    "from keras.optimizers import SGD as keras_SGD, RMSprop as keras_RMSProp\n",
    "from keras.callbacks import ModelCheckpoint, Callback as keras_callback\n",
    "from sklearn import model_selection as ms\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.layers.convolutional import ZeroPadding2D\n",
    "import os, h5py\n",
    "from timeit import default_timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LossHistory(keras_callback):\n",
    "    def __init__(self, filename, epoch_num):\n",
    "        super(keras_callback, self).__init__()\n",
    "        \n",
    "        self.batch_count = 0\n",
    "        self.epoch_num = epoch_num\n",
    "        self.filename = filename\n",
    "        self.batch_time = None\n",
    "        \n",
    "        self.f = h5py.File(filename, 'w')\n",
    "        \n",
    "        config = self.f.create_group('config')\n",
    "        config.attrs[\"total_epochs\"] = self.epoch_num\n",
    "        \n",
    "        cost = self.f.create_group('cost')\n",
    "        loss = cost.create_dataset('loss', (self.epoch_num,))\n",
    "        loss.attrs['time_markers'] = 'epoch_freq'\n",
    "        loss.attrs['epoch_freq'] = 1\n",
    "        train = cost.create_dataset('train', (10,), maxshape=(None,)) # No way to get total number of batches before running the model\n",
    "        train.attrs['time_markers'] = 'minibatch'\n",
    "        \n",
    "        t = self.f.create_group('time')\n",
    "        loss = t.create_dataset('loss', (self.epoch_num,))\n",
    "        train = t.create_group('train')\n",
    "        start_time = train.create_dataset(\"start_time\", (1,))\n",
    "        start_time.attrs['units'] = 'seconds'\n",
    "        end_time = train.create_dataset(\"end_time\", (1,))\n",
    "        end_time.attrs['units'] = 'seconds'\n",
    "        train_batch = t.create_dataset('train_batch', (10,), maxshape=(None,)) # Same as above\n",
    "        \n",
    "        time_markers = self.f.create_group('time_markers')\n",
    "        time_markers.attrs['epochs_complete'] = self.epoch_num\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.f['.']['time']['train']['start_time'] = default_timer()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.f['.']['cost']['loss'][epoch] = np.float32(logs.get('val_loss'))\n",
    "        self.f['.']['time_markers']['minibatch'] = np.float32(self.batch_count)\n",
    "        \n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        self.batch_time = default_timer()\n",
    "    \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.f['.']['cost']['train'][self.batch_count] = np.float32(logs.get('loss'))\n",
    "        self.f['.']['time']['train_batch'][self.batch_count] = (default_timer() - self.batch_time)\n",
    "        self.batch_count += 1\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        self.f['.']['time']['train']['end_time'] = default_timer()\n",
    "        self.f['.']['config'].attrs[\"total_minibatches\"] = self.batch_count\n",
    "        self.f['.']['time_markers'].attrs['minibatches_complete'] = self.batch_count\n",
    "        self.f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to dynamically change keras backend\n",
    "from importlib import reload\n",
    "def set_keras_backend(backend):\n",
    "    if K.backend() != backend:\n",
    "        os.environ['KERAS_BACKEND'] = backend\n",
    "        reload(K)\n",
    "        assert K.backend() == backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import platform\n",
    "backends = [\"theano\", \"tensorflow\"]\n",
    "if platform != \"darwin\":\n",
    "    backends.append(\"cntk\")\n",
    "\n",
    "for b in backends:\n",
    "    set_keras_backend(b)\n",
    "\n",
    "    # Load and process images\n",
    "    enc = OneHotEncoder(sparse=False)\n",
    "    x_train, x_valid, y_train, y_valid = ms.train_test_split(trainImages, trainLabels, test_size=0.2, random_state=542)\n",
    "    keras_train_x = np.vstack([np.expand_dims(image.img_to_array(x), axis=0).astype('float32')/255 for x in x_train])\n",
    "    keras_valid_x = np.vstack([np.expand_dims(image.img_to_array(x), axis=0).astype('float32')/255 for x in x_valid])\n",
    "    keras_test_x = np.vstack([np.expand_dims(image.img_to_array(x), axis=0).astype('float32')/255 for x in testImages])\n",
    "    keras_train_y = to_categorical(y_train, 43)\n",
    "    keras_valid_y = to_categorical(y_valid, 43)\n",
    "    keras_test_y = to_categorical(testLabels, 43)\n",
    "\n",
    "    # Build model\n",
    "    keras_gaussInit = RandomNormal(mean=0.0, stddev=0.01, seed=542)\n",
    "    layer_name_prefix = b+\"_\"\n",
    "\n",
    "    keras_model = Sequential()\n",
    "    keras_model.add(keras_Conv(64, (5, 5), kernel_initializer=keras_gaussInit, strides=(2, 2), bias_initializer=keras_Constant(0.0), activation=\"relu\", input_shape=(resize_size[0], resize_size[1], 3), name=layer_name_prefix+\"conv1\"))\n",
    "    keras_model.add(keras_MaxPooling(pool_size=(2, 2), name=layer_name_prefix+\"pool1\"))\n",
    "    keras_model.add(keras_Conv(256, (3, 3), kernel_initializer=keras_gaussInit, strides=(1, 1), padding=\"same\", bias_initializer=keras_Constant(0.0), activation=\"relu\", name=layer_name_prefix+\"conv2\"))\n",
    "    keras_model.add(keras_MaxPooling(pool_size=(2, 2), name=layer_name_prefix+\"pool2\"))\n",
    "    keras_model.add(keras_AveragePooling(name=layer_name_prefix+\"global_pool\"))\n",
    "#     keras_model.add(Flatten(name=layer_name_prefix+\"flatten\")) # An extra layer to flatten the previous layer in order to connect to fully connected layer\n",
    "#     keras_model.add(Dense(4096, kernel_initializer=keras_gaussInit, bias_initializer=keras_Constant(0.0), activation=\"relu\", name=layer_name_prefix+\"fc1\"))\n",
    "    keras_model.add(keras_Dropout(0.5, name=layer_name_prefix+\"drop_out\"))\n",
    "    keras_model.add(Dense(43, kernel_initializer=keras_gaussInit, bias_initializer=keras_Constant(0.0), activation=\"softmax\", name=layer_name_prefix+\"fc2\"))\n",
    "    keras_model.summary()\n",
    "\n",
    "    keras_optimizer = keras_SGD(lr=0.01, decay=1.6e-8, momentum=0.9)\n",
    "#     keras_optimizer = keras_RMSProp(lr=0.01, decay=0.95)\n",
    "    keras_cost = \"categorical_crossentropy\"\n",
    "    keras_model.compile(loss=keras_cost, optimizer=keras_optimizer, metrics=[\"acc\"])\n",
    "\n",
    "    checkpointer = ModelCheckpoint(filepath=root+\"/saved_models/keras_\"+b+\"_weights.hdf5\",\n",
    "                                       verbose=1, save_best_only=True)\n",
    "    losses = LossHistory(root+\"/callback_data_{}.h5\".format(b), epoch_num)\n",
    "\n",
    "    start = time.time()\n",
    "    keras_model.fit(keras_train_x, keras_train_y,\n",
    "                  validation_data=(keras_valid_x, keras_valid_y),\n",
    "                  epochs=epoch_num, batch_size=batch_size, callbacks=[checkpointer, losses], verbose=1, shuffle=True)\n",
    "    print(\"{} training finishes in {:.2f} seconds.\".format(b, time.time() - start))\n",
    "    \n",
    "#     train_cost['keras_{}'.format(b)] = pd.Series(\"f['.']['cost']['train'][()]\")\n",
    "\n",
    "    keras_model.load_weights(root+\"/saved_models/keras_\"+b+\"_weights.hdf5\")\n",
    "    keras_predictions = [np.argmax(keras_model.predict(np.expand_dims(feature, axis=0))) for feature in keras_test_x]\n",
    "\n",
    "    # report test accuracy\n",
    "    keras_test_accuracy = 100*np.sum(np.array(keras_predictions)==np.argmax(keras_test_y, axis=1))/len(keras_predictions)\n",
    "    print('{} test accuracy: {:.1f}%'.format(b, keras_test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as utils\n",
    "import torch.nn.init as torch_init\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        # Build model\n",
    "        self.conv = torch.nn.Sequential()\n",
    "        self.conv.add_module(\"torch_conv1\", torch.nn.Conv2d(3, 64, kernel_size=(5, 5), stride=2))\n",
    "        self.conv.add_module(\"torch_pool1\", torch.nn.MaxPool2d(kernel_size=2))\n",
    "        self.conv.add_module(\"torch_relu1\", torch.nn.ReLU())\n",
    "        self.conv.add_module(\"torch_conv2\", torch.nn.Conv2d(64, 256, kernel_size=(3, 3), stride=1, padding=1))\n",
    "        self.conv.add_module(\"torch_pool2\", torch.nn.MaxPool2d(kernel_size=2))\n",
    "        self.conv.add_module(\"torch_relu2\", torch.nn.ReLU())\n",
    "        self.conv.add_module(\"torch_global_pool\", torch.nn.AvgPool2d(kernel_size=5))\n",
    "        \n",
    "        self.csf = torch.nn.Sequential()\n",
    "        self.csf.add_module(\"torch_fc1\", torch.nn.Linear(256, 4096))\n",
    "        self.csf.add_module(\"torch_relu3\", torch.nn.ReLU())\n",
    "        self.csf.add_module(\"torch_dropout1\", torch.nn.Dropout(0.5))\n",
    "        self.csf.add_module(\"torch_fc2\", torch.nn.Linear(4096, 43))\n",
    "        \n",
    "        # Initialize conv layers and fc layers\n",
    "        torch_init.normal(self.conv.state_dict()[\"torch_conv1.weight\"], mean=0, std=0.01)\n",
    "        torch_init.constant(self.conv.state_dict()[\"torch_conv1.bias\"], 0.0)\n",
    "        torch_init.normal(self.conv.state_dict()[\"torch_conv2.weight\"], mean=0, std=0.01)\n",
    "        torch_init.constant(self.conv.state_dict()[\"torch_conv2.bias\"], 0.0)\n",
    "        torch_init.normal(self.csf.state_dict()[\"torch_fc1.weight\"], mean=0, std=0.01)\n",
    "        torch_init.constant(self.csf.state_dict()[\"torch_fc1.bias\"], 0.0)\n",
    "        torch_init.normal(self.csf.state_dict()[\"torch_fc2.weight\"], mean=0, std=0.01)\n",
    "        torch_init.constant(self.csf.state_dict()[\"torch_fc2.bias\"], 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv.forward(x)\n",
    "        x = x.view(-1, 256)\n",
    "        return self.csf.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch_train_x = torch.stack([torch.Tensor(i.swapaxes(0,2).astype(\"float32\")/255) for i in x_train])\n",
    "torch_train_y = torch.LongTensor(y_train)\n",
    "torch_valid_x = torch.stack([torch.Tensor(i.swapaxes(0,2).astype(\"float32\")/255) for i in x_valid])\n",
    "torch_valid_y = torch.LongTensor(y_valid)\n",
    "torch_test_x = torch.stack([torch.Tensor(i.swapaxes(0,2).astype(\"float32\")/255) for i in testImages])\n",
    "torch_test_y = torch.LongTensor(testLabels)\n",
    "\n",
    "torch_tensor_train_set = utils.TensorDataset(torch_train_x, torch_train_y)\n",
    "torch_train_set = utils.DataLoader(torch_tensor_train_set, batch_size=batch_size, shuffle=True)\n",
    "torch_tensor_valid_set = utils.TensorDataset(torch_valid_x, torch_valid_y)\n",
    "torch_valid_set = utils.DataLoader(torch_tensor_valid_set, batch_size=batch_size, shuffle=True)\n",
    "torch_tensor_test_set = utils.TensorDataset(torch_test_x, torch_test_y)\n",
    "torch_test_set = utils.DataLoader(torch_tensor_test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "torch_model = ConvNet()\n",
    "optimizer = optim.SGD(torch_model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    torch_model.train()\n",
    "    for batch_idx, (data, target) in enumerate(torch_train_set):\n",
    "#         if args.cuda:\n",
    "#             data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = torch_model(data)\n",
    "        cost = torch.nn.CrossEntropyLoss(size_average=True)\n",
    "        loss = cost(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(torch_train_set.dataset),\n",
    "                100. * batch_idx / len(torch_train_set), loss.data[0]))\n",
    "def test():\n",
    "    torch_model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in torch_test_set:\n",
    "#         if args.cuda:\n",
    "#             data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = torch_model(data)\n",
    "        cost = torch.nn.CrossEntropyLoss(size_average=False)\n",
    "        test_loss += cost(output, target).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(torch_test_set.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(torch_test_set.dataset),\n",
    "        100. * correct / len(torch_test_set.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, epoch_num + 1):\n",
    "    train(epoch)\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.DEBUG)  # logging to stdout\n",
    "\n",
    "epoch_num = 5\n",
    "batch_size = 128\n",
    "resize_size = (49, 49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mx_train_x = mx.nd.array([i.swapaxes(0,2).astype(\"float32\")/255 for i in x_train])\n",
    "mx_valid_x = mx.nd.array([i.swapaxes(0,2).astype(\"float32\")/255 for i in x_valid])\n",
    "mx_test_x = mx.nd.array([i.swapaxes(0,2).astype(\"float32\")/255 for i in testImages])\n",
    "mx_train_y = mx.nd.array(y_train, dtype=np.float32) # No need of one_hot\n",
    "mx_valid_y = mx.nd.array(y_valid, dtype=np.float32)\n",
    "mx_test_y = mx.nd.array(testLabels, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The iterators have input name of 'data' and output name of 'softmax_label' if not particularly specified\n",
    "mx_train_set = mx.io.NDArrayIter(mx_train_x, mx_train_y, batch_size, shuffle=True)\n",
    "mx_valid_set = mx.io.NDArrayIter(mx_valid_x, mx_valid_y, batch_size)\n",
    "mx_test_set = mx.io.NDArrayIter(mx_test_x, mx_test_y, batch_size)\n",
    "\n",
    "# Print the shape and type of training set lapel\n",
    "# mx_train_set.provide_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = mx.sym.Variable('data')\n",
    "mx_conv1 = mx.sym.Convolution(data = data, name='mx_conv1', num_filter=64, kernel=(5,5), stride=(2,2))\n",
    "mx_act1 = mx.sym.Activation(data = mx_conv1, name='mx_relu1', act_type=\"relu\")\n",
    "mx_mp1 = mx.sym.Pooling(data = mx_act1, name = 'mx_pool1', kernel=(2,2), stride=(2,2), pool_type='max')\n",
    "mx_conv2 = mx.sym.Convolution(data = mx_mp1, name='mx_conv2', num_filter=512, kernel=(3,3), stride=(1,1), pad=(1,1))\n",
    "mx_act2 = mx.sym.Activation(data = mx_conv2, name='mx_relu2', act_type=\"relu\")\n",
    "mx_mp2 = mx.sym.Pooling(data = mx_act2, name = 'mx_pool2', kernel=(2,2), stride=(2,2), pool_type='max')\n",
    "mx_fl = mx.sym.Flatten(data = mx_mp2, name=\"mx_flatten\")\n",
    "mx_fc1 = mx.sym.FullyConnected(data = mx_fl, name='mx_fc1', num_hidden=2048)\n",
    "mx_drop = mx.sym.Dropout(data = mx_fc1, name='mx_dropout', p=0.5)\n",
    "mx_fc2 = mx.sym.FullyConnected(data = mx_drop, name='mx_fc2', num_hidden=43)\n",
    "mx_softmax = mx.sym.SoftmaxOutput(data = mx_fc2, name ='softmax')\n",
    "\n",
    "# Print the names of arguments in the model\n",
    "# mx_softmax.list_arguments() # Make sure the input and the output names are consistent of those in the iterator!!\n",
    "\n",
    "# Print the size of the model\n",
    "# mx_softmax.infer_shape(data=(1,3,49,49))\n",
    "\n",
    "# Draw the network\n",
    "# mx.viz.plot_network(mx_softmax, shape={\"data\":(batch_size, 3, resize_size[0], resize_size[1])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MxCustomInit(mx.initializer.Initializer):\n",
    "    def __init__(self, idict):\n",
    "        super(MxCustomInit, self).__init__()\n",
    "        self.dict = idict\n",
    "        np.random.seed(seed=1)\n",
    "\n",
    "    def _init_weight(self, name, arr):\n",
    "        if name in self.dict.keys():\n",
    "            dictPara = self.dict[name]\n",
    "            for(k, v) in dictPara.items():\n",
    "                arr = np.random.normal(0, v, size=arr.shape)\n",
    "\n",
    "    def _init_bias(self, name, arr):\n",
    "        if name in self.dict.keys():\n",
    "            dictPara = self.dict[name]\n",
    "            for(k, v) in dictPara.items():\n",
    "                arr[:] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mx_nor_dict = {'normal': 0.01}\n",
    "mx_cons_dict = {'constant': 0.0}\n",
    "mx_init_dict = {}\n",
    "for layer in mx_softmax.list_arguments():\n",
    "    hh = layer.split('_')\n",
    "    if hh[-1] == 'weight':\n",
    "        mx_init_dict[layer] = mx_nor_dict\n",
    "    elif hh[-1] == 'bias':\n",
    "        mx_init_dict[layer] = mx_cons_dict\n",
    "# print(mx_init_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a trainable module on CPU\n",
    "mx_model = mx.mod.Module(context = mx.cpu(), symbol = mx_softmax)\n",
    "\n",
    "# Currently no solution to reproducibility. Eyes on issue 47.\n",
    "mx_model.fit(mx_train_set, # train data\n",
    "             eval_data = mx_valid_set, # validation data\n",
    "             num_epoch = epoch_num,\n",
    "             initializer = MxCustomInit(mx_init_dict),\n",
    "             optimizer = 'sgd',\n",
    "             optimizer_params = {'learning_rate': 0.1, 'momentum': 0.9},\n",
    "             eval_metric ='acc', # report accuracy during training\n",
    "             batch_end_callback = mx.callback.Speedometer(batch_size, 10)) # output progress for each 10 data batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = mx_model.score(mx_test_set, ['acc'])\n",
    "print(\"Accuracy score is %f\" % (score[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
