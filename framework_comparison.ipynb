{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Comparison of Deep Learning Frameworks\n",
    "\n",
    "## Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv, time, os.path\n",
    "from six.moves import cPickle\n",
    "from sklearn import model_selection as ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for reading the images\n",
    "# arguments: path to the traffic sign data, for example './GTSRB/Training'\n",
    "# returns: list of images, list of corresponding labels \n",
    "def readTrafficSigns(rootpath, size, training=True):\n",
    "    '''Reads traffic sign data for German Traffic Sign Recognition Benchmark.\n",
    "\n",
    "    Arguments: path to the traffic sign data, for example './GTSRB/Training'\n",
    "    Returns:   list of images, list of corresponding labels'''\n",
    "    images = [] # images\n",
    "    labels = [] # corresponding labels\n",
    "    # loop over all 43 classes\n",
    "    if training:\n",
    "        for c in range(0,43):\n",
    "            prefix = rootpath + '/' + format(c, '05d') + '/' # subdirectory for class\n",
    "            gtFile = open(prefix + 'GT-'+ format(c, '05d') + '.csv') # annotations file\n",
    "            gtReader = csv.reader(gtFile, delimiter=';') # csv parser for annotations file\n",
    "            gtReader.next() # skip header\n",
    "            # loop over all images in current annotations file\n",
    "            for row in gtReader:\n",
    "#                 image = Image.open(prefix + row[0]).convert('L') # Load an image and convert to grayscale\n",
    "                image = Image.open(prefix + row[0])\n",
    "                box = (int(row[3]), int(row[4]), int(row[5]), int(row[6])) # Specify ROI box\n",
    "                image = image.crop(box) # Crop the ROI\n",
    "                image = image.resize(size) # Resize images\n",
    "                images.append(np.asarray(image).astype('uint8')) # the 1th column is the filename, while 3,4,5,6 are the vertices of ROI\n",
    "                labels.append(int(row[7])) # the 8th column is the label\n",
    "            gtFile.close()\n",
    "    else:\n",
    "        gtFile = open(rootpath + \"/../../GT-final_test.csv\") # annotations file\n",
    "        gtReader = csv.reader(gtFile, delimiter=';') # csv parser for annotations file\n",
    "        gtReader.next() # skip header\n",
    "        # loop over all images in current annotations file\n",
    "        for row in gtReader:\n",
    "#             image = Image.open(rootpath + '/' + row[0]).convert('L') # Load an image and convert to grayscale\n",
    "            image = Image.open(rootpath + '/' + row[0]) # Color version\n",
    "            box = (int(row[3]), int(row[4]), int(row[5]), int(row[6])) # Specify ROI box\n",
    "            image = image.crop(box) # Crop the ROI\n",
    "            image = image.resize(size) # Resize images\n",
    "            images.append(np.asarray(image).astype('uint8')) # the 1th column is the filename, while 3,4,5,6 are the vertices of ROI\n",
    "            labels.append(int(row[7])) # the 8th column is the label\n",
    "        gtFile.close()\n",
    "        \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zhongyilin/Desktop/GTSRB/try\n"
     ]
    }
   ],
   "source": [
    "from sys import platform\n",
    "global root\n",
    "global epoch_num\n",
    "if platform == \"darwin\":\n",
    "    root = \"/Users/moderato/Downloads/GTSRB/try\"\n",
    "else:\n",
    "    root = \"/home/zhongyilin/Desktop/GTSRB/try\"\n",
    "print(root)\n",
    "train_dir = root + \"/Final_Training/Images\"\n",
    "test_dir = root + \"/Final_Test/Images\"\n",
    "resize_size = (49, 49)\n",
    "epoch_num = 60\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainImages list length 39209, trainLabels list length 39209\n",
      "testImages list length 12630, testLabels list length 12630\n",
      "(49, 49, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAF/NJREFUeJzt3X1sneV5BvDr9vHx8XdiJ47jxAlOQggktAQUEgakhEA2\nPqpC164tW6dMQkOrOolKndqwStXYJo1pU9U/Ok2ia9dU7VpRQUVgbJCGAOMrJIQA+SBxEjBJbMfO\nh7+/z7n3h4+pHSXv9YbYxyd6rp9k2e95L7/v4+Nz+7WP7/M85u4QkfAUTPcARGR6qPhFAqXiFwmU\nil8kUCp+kUCp+EUCpeIXCZSKXyRQl1T8ZnaXmR00s8NmtmmyBiUiU88+bYefmSUAHAKwAcBxADsB\nPODu+y/0ORWplM8uK4k+cAH/eWQFhXyAsTLRu4sKjB6ir+sszaQqS2lmwPnX7TF+Vmcy/PuZydAI\nzv3K3fl9kU4P08xIeoRm4pwrEyMD8C/USCYR5zTGQxYnE+OxD09H7u7pHcDA4FCcUSNGhVzQagCH\n3f0oAJjZrwHcB+CCxT+7rASP3rku8qBWWkxPbKXVPFM2i2a8JBG5fyHZDwDvb/sNzSy88waaOZgh\nPxQBZJz/EOnt48U/0BejKDITH4jpDH9gnu1o45nOUzQzOMzv96ER/tBNex/NFKM/cn9FIR+Lx8gk\nUik+lmKe8UxH5P5nnt9FjzHmUn7tnw/g2Ljt49nbROQyMOVP+JnZQ2a2y8x2dQ8OTfXpRCSmSyn+\nEwAWjNuuz942gbs/7u6r3H1VRaroEk4nIpPpUop/J4ClZrbIzIoAfA3AlskZlohMtU/9hJ+7j5jZ\nXwN4HkACwE/dfd+kjUxEptSlPNsPd38OwHOTNBYRySF1+IkE6pKu/BdruCCB5oqKyExpZoAep6yP\nN5O0DfP/d7f39kbu39/dQo9RUsHPc6V/SDN4n59r0dKbaabh6ptoJjNYRjM+NPG+sQT/Oi1xFT93\njP+JZwpn00zjId5TcLjxdX6cziPRx+iK/r86AJTEaM4pTfBMqpT3epSUJSP3x+jf+oSu/CKBUvGL\nBErFLxIoFb9IoFT8IoFS8YsESsUvEigVv0igctrk0zU0iBc+OhyZuSLDX/a7Zs5CmrnmljU0c/Sj\nvZH7Wztb6TGWN1xDMylU0cy1ztszZnTz+6b99Vdpxgf4RC8pTJwxJh2jfSQD3ggU3VY1Ks5ELPXl\nPFNY0EUzV224NXL/f+7lj4HWfTxTXc3vm4oqfh+zl8XH+R6M0ZVfJFAqfpFAqfhFAqXiFwmUil8k\nUCp+kUCp+EUCpeIXCVROm3wMjqRFL9c0XBw9UwkAHOrns7jsfe5nNFNQGD1r0E0NfEaZG67iq/EM\nHOWzE6Vxmmba2nnTSjrJ77/hGOt1dZyzjFucZasKY8xok4lxHO/nsxqd6TlGMyUF/H7vbTkauf8b\n675Ej7F7Np9Zavfep2imMM2bhUbIsm4Xs/qervwigVLxiwRKxS8SKBW/SKBU/CKBUvGLBErFLxIo\nFb9IoHLa5OMZx0h/9EwkpzMpepy+At6kciU/DG6cHz0jUH1NPT1Gy16+MHF3M1/yaVaSz9KTKOTN\nJOWVi2jGZvBMxbzaiedO8/ENdffRzGAPb7zp6+D310BnnGW0emimtzX6OF7AZ0ZaMZ8vU9bcyx9L\nbW38Qes2GB2w+CWtK79IoFT8IoFS8YsESsUvEigVv0igVPwigaLFb2Y/NbM2M9s77rZqM9tqZo3Z\n93xVChHJK3H+KfgzAD8C8PNxt20CsM3dHzOzTdnt78Y6Yzp6d2KABAAUDnXSzJy5NTQzl/yfv/lj\nfp7utpM0U1jCZ7DoK+a9C1et/izNLFv9pzRTVMknIEnMmJoWkAz/9zy6Os7SzP4dO2jmwI6naWZ4\npD9yf0fTO/QYhcPRq1ABwK2L76CZ94v46k/vHY8eT+YiWnfold/dXwFw5pyb7wOwOfvxZgD3xz6j\niOSFT/s3f627j8211AqgNiosIvnnkn+3c3c3swvOHGZmDwF4CABSRXx+ORHJjU975T9pZnUAkH1/\nwRk13f1xd1/l7quKkjl9KYGIRPi0xb8FwMbsxxsB8GdWRCSvxPlX368AvAFgmZkdN7MHATwGYIOZ\nNQK4M7stIpcR+nu4uz9wgV38fxcikrfU4ScSqJw+A1eYMNRURU9YUFvMJzSYX95AM9ctW0MzH37c\nGLm/+3gTPcYVM2fQjFfxBsgVa2+jmcVrP0czbQWzaKYgEePbfs5lIcZiPLFWi3G+CBIGKmLcX7Xr\naCY5u4xmPnj9rcj9hV0H6DGOfcwndJnpH9DM0gXX0kxTOrphLJGMsSRSlq78IoFS8YsESsUvEigV\nv0igVPwigVLxiwRKxS8SKBW/SKBy2uRTVFiA+dUlkZky8JVhFi/iM9qcOlFJM13Ho2dxSaUS9BiZ\nypk0c81a3gm9+Pb7+LnSvGmlfISv6pPkXxaS57z6+gyfYAlnYzT59MSYyaen8wTNLEjxr3PV6rU0\nU5CObqzZ/caP6TFG+vh4Tx7bTzN9Iy00s3jegsj9b17Eq+Z15RcJlIpfJFAqfpFAqfhFAqXiFwmU\nil8kUCp+kUCp+EUCldu5tDMO9A9GRmbOLqWHMT9NM2eaD9LMDIteaiuTih4rACy9kc/As2TNn9HM\n5mf4BMgz0Esz6OANJzevXUkzyVT9hG1fwJeS+u3252jm9RefpJnTzQM0Yy28GexbX7mbZlbfEt0w\nNn9oMT1G67YLzlz/iTLw78uJlmaaKZ5ZHh3IxL+e68ovEigVv0igVPwigVLxiwRKxS8SKBW/SKBU\n/CKBUvGLBCqnTT4ZAP0WPY1MuiDGdDDgTT7Vs/iUMcPt0TOn1C+7mh5jyWre5PPSW2do5plnXqSZ\n/u53aGaw5wjNNDYvpZm7135zwnZ5ijf5NO5+iWYWln9MM3+0/gs0M9DPvzf/vuV7NHM6MSdy/4Z1\nf0+P0ds6l2aOvPELmqkw/jip7Itu8inMxJimKUtXfpFAqfhFAqXiFwmUil8kUCp+kUCp+EUCpeIX\nCRQtfjNbYGbbzWy/me0zs4ezt1eb2VYza8y+r5r64YrIZInT5DMC4NvuvtvMKgC8bWZbAfwFgG3u\n/piZbQKwCcB3I49UYPDi6FOWzeA/Qzo7+2imo6WTZhbOiF76aMGy2+kx/uv5F2jmw5boJcoAYO3n\n1tFMzXze/HLgSCPNHHmXNwsdfG/i+lzLZvHmq8HT0cufAcDV9RU0c8dy3oSUathAMy8d+g3NPL8j\n+r64ZS3/3i2/dj7NtL9fTDPD3XxNtJldTZH7E2k++9QYeuV39xZ33539uBvAAQDzAdwHYHM2thnA\n/bHPKiLT7qLae82sAcD1AHYAqHX3sf7YVgC1F/ichwA8BADlpalPO04RmWSxn/Azs3IATwL4lrt3\njd/n7g7gvL8Xuvvj7r7K3VeVFF/EEqIiMqViFb+ZJTFa+L9096eyN580s7rs/joAfApTEckbcZ7t\nNwA/AXDA3X8wbtcWABuzH28EwOeeFpG8Eedv/lsA/DmA981sT/a2vwXwGIAnzOxBAE0AvjI1QxSR\nqUCL391fBWAX2H3H5A5HRHJFHX4igcrtTD4jIxjoaI/M1F6ziB7nTGsrzXhBGc0Ml0c3+bR0k6WR\nAKy9cx3NzDvGZx46eozP4pKoWEYzt93Pfxn7eAa///Zt3zZh+4X3fkk/56zxJp/uWr7M1twV/Os8\nwlf0QuVSvixZ647o2ZyK0nz5uIrZlTSTLOdNPpl+fi0+3fVR5P6RNL9/x+jKLxIoFb9IoFT8IoFS\n8YsESsUvEigVv0igVPwigVLxiwQqp00+ZaWVWHNd9AwstRUN9Dj96R6a6SrgM/nULlseuX/pyjX0\nGB0x7sJFS/nMNIuXz6CZziIawX+/9T7NHHp1M808sPKWCdtf3/BF+jmP/OhlmukajtHIEmPFtjMJ\nHuqtqKaZjoHol5l7H58Zp6aOz/aTLOfNQt7BG4Hqr4huKCpKxX/ZvK78IoFS8YsESsUvEqic/s1f\nWFiM6prov7OH0rPpcYbT79JMWWEXzfS2nIzc33SILyddc8MqmoFd6BXRv1cUY3rD9mP81SwtH/yO\nZqpn8Rl+G67/wwnb/Sn+op3BEr489KlO/jUUxbgmpZL8Pi2vnEUzlol+8Zb38xfKmA/TTEUF/5u/\n1PnXhPbj0ftH9MIeESFU/CKBUvGLBErFLxIoFb9IoFT8IoFS8YsESsUvEqicNvkMDY/g4xPRs/fO\nnMtfmJAs4j+zEiO8qaKoP7oR6KqFM+kxOhBjSWTjL9g4+mF0wxEAnDrJlybvbebNTYtnXUMzidn1\nE7Z7imNcJ6pqaKT9+Ic0YyP8/krxyZlRXMbHU+DRB/L+GK8ycl5G/f38cVJ6/uUuJ5iXjP4+JGM0\nlI3RlV8kUCp+kUCp+EUCpeIXCZSKXyRQKn6RQKn4RQKl4hcJVE6bfIaHRtBy4lRkJt3eRI9Tm+LD\nTg/zzNBA9CzAhw7tpMeYUzOXZk4Pp2mmPcmn8nn7zZdopqylm2bWXHsDzVx9TpPPnjT/Gkaq+X1x\ndj+/3pTFWBa7N3plbQDA3PJamtmfiJ41+cwAb5rpbOczRRcO8Bl2kkX8MdtHmnwy8Xt8dOUXCZWK\nXyRQtPjNrNjM3jKzd81sn5k9mr292sy2mllj9n3V1A9XRCZLnCv/IID17n4dgJUA7jKzmwBsArDN\n3ZcC2JbdFpHLBC1+HzX2zFgy++YA7gMwtu7TZgD3T8kIRWRKxPqb38wSZrYHQBuAre6+A0Ctu489\n59oKgD+1KiJ5I1bxu3va3VcCqAew2syuPWe/A+d/MbKZPWRmu8xsV98gX/hBRHLjop7td/cOANsB\n3AXgpJnVAUD2fdsFPudxd1/l7qtKU3w1UxHJDdpVYGY1AIbdvcPMSgBsAPDPALYA2Ajgsez7p9mx\nkkWG+nnRp1wybx4ddEfzCZoZ6OGz3gySmXwMHfQYfQO8qeZMgi8V/fNtv6GZzjefpJl/uPtumrnt\nptU00/i7X0/YTt55E/2c6quvppnD//MOzby8ZSvNFCy/kmb2vvkKzSQLj0XuL6/hzVenT/HHifX2\n0kxhEe/QuXrNPZH7i589SI/xyfliZOoAbDazBEZ/U3jC3Z81szcAPGFmDwJoAvCV2GcVkWlHi9/d\n3wNw/XluPw3gjqkYlIhMPXX4iQRKxS8SKBW/SKBU/CKBUvGLBErFLxKonM7k09PXhdf2RDdwdJ/l\nS2Rdd8USmikaKOLjaY+eDubYkV30GNevWE8zvX18ppeXt/EGnlvnnKWZ147soJnt/7qPZkaGDk/Y\nXpzhsxo9uP5Rmrn5i4/QzP8+8wOaaX7uxzTTWshnz/na+jWR++fU8MfRu7sO08xIH2/yqZjBr8Xp\nftIlm4l/PdeVXyRQKn6RQKn4RQKl4hcJlIpfJFAqfpFAqfhFAqXiFwlUTpt8HEkMJKKXdDreytdh\n+sz8SpoZGk7STKZgMHosR07TY5S/9TrN9M/lM72sWrmSZpp2vkozJb3RS5ABQFkZn51m6dIbJ2zP\nSfH5WRe1nHcmtwlu+2P+db5efC/NLLhuAc0MlEQvxQUAVSXRTWU7X9lOj9F0kM8YVJfkj4Hi4vNO\ngznBlQsrIvenihL0GGN05RcJlIpfJFAqfpFAqfhFAqXiFwmUil8kUCp+kUCp+EUCldMmH7MkkqTJ\nx5Mxlr8aiW50AIAZtctppqUnusnHR3jTxdGdvMmn7sbo8wDAP335yzST+uo3aKYowZd8SiR5I0hh\ncuJDI5Xgn1NSyB9Owwk+u87NfxK9JBUAIM0jIxme2bO9PXJ/86536TEKcZRmupJ8kdqi+bxxaX8m\nuvGsHyP0GGN05RcJlIpfJFAqfpFAqfhFAqXiFwmUil8kUCp+kUCp+EUCldMmH2AYBTgZHeGrI2Eo\nxZetKp1dRjOV/aWR+0838cHMTPOmms7d79BMfZIswwRg/pqbaeZUAZ/lKO18STTYxOtCQRGf/WfQ\n+MNpMM2bhVK9vCmq1HgHz85XXqOZw69EN2nNGW6ixzhjfCmuqiU30Exn5SKa2dsSPZ7uoWF6jDG6\n8osEKnbxm1nCzN4xs2ez29VmttXMGrPvq6ZumCIy2S7myv8wgAPjtjcB2ObuSwFsy26LyGUiVvGb\nWT2AewH8x7ib7wOwOfvxZgD3T+7QRGQqxb3y/xDAdwCMf5al1t3H5tluBcDndhaRvEGL38w+D6DN\n3d++UMbdHcB5X/9qZg+Z2S4z2zU4xF/OKSK5EedffbcA+IKZ3QOgGEClmf0CwEkzq3P3FjOrA3De\nFRvc/XEAjwNA1YwZ/AXyIpITtPjd/REAjwCAma0D8Dfu/nUz+xcAGwE8ln3/ND0WhpFGa2SmYtYc\nOmgv5qufDA2doplrFi2J3P9RIV/x5ePjjTSTGuATObz3f8/STPvJC/7y9Yl5y+6kmapZ19FM9eya\nCduFw3z2jGSqnGZ6z3TSTP9ZvmrTu/t308yB/W/QTCodPTlGZ4aPd+YS/pitWMLv85G+hTTTc2Bb\n5P7MIO+RGHMp/+d/DMAGM2sEcGd2W0QuExfV4efuLwF4KfvxaQB3TP6QRCQX1OEnEigVv0igVPwi\ngVLxiwRKxS8SKBW/SKByOplHUVEKdfXRExbMmR+9og8AdAzwRoYSPr8CVjREN1VcseFWeowX97xM\nMyf3vEIz5TFWBzq97xjPHNxCM4nCV2mmtHTiRCelZXxylIHBGBNJxMiUFvCJOoZjZBak+H3a2R99\n/atb0kCPUdjAm8Gah3lre3WK38d/ee9fRe7/7VN8haExuvKLBErFLxIoFb9IoFT8IoFS8YsESsUv\nEigVv0igVPwigcppk086nUZPZ3T3zVA3b4b4/t/9I80cbzpOM0eboptASmcso8d44EtX0cye2hqa\nefPV52nGB8/QDGKs2FKUbufnGpq4ElEPn9AGbrypJlUY43qTTPJMMV9BCCm+CtLcOVdE7l+whK9u\nVHXNbJq57TNfpZnD+4pppn5F9P6i8gp6jDG68osESsUvEigVv0igVPwigVLxiwRKxS8SKBW/SKBU\n/CKBymmTT6IgicrS6MV8h3u66HFee+YJmjlRzL+0F44kIvdXHn2PHuPR+2+nmZW330szBfP4Iscn\nD/IlqvqbeTdOVzO/j8vRM2E7jT76OSjg1xJLRN/nAFBVw5e/KouRaWrbTzN1DUWR+69a8Rl6DKy4\ngWdilNqVpIFnsunKLxIoFb9IoFT8IoFS8YsESsUvEigVv0igVPwigVLxiwTK3PnsK5N2MrN2AE0A\nZgM4lbMTT47Lbcwa79TLxzFf4e586ijkuPg/OanZLndflfMTX4LLbcwa79S7HMc8nn7tFwmUil8k\nUNNV/I9P03kvxeU2Zo136l2OY/7EtPzNLyLTT7/2iwQq58VvZneZ2UEzO2xmm3J9fsbMfmpmbWa2\nd9xt1Wa21cwas++rpnOM45nZAjPbbmb7zWyfmT2cvT2fx1xsZm+Z2bvZMT+avT1vxwwAZpYws3fM\n7Nnsdl6Pl8lp8ZtZAsC/AbgbwHIAD5jZ8lyOIYafAbjrnNs2Adjm7ksBbMtu54sRAN929+UAbgLw\nzex9ms9jHgSw3t2vA7ASwF1mdhPye8wA8DCAA+O283280dw9Z28A/gDA8+O2HwHwSC7HEHOcDQD2\njts+CKAu+3EdgIPTPcaIsT8NYMPlMmYApQB2A1iTz2MGUI/RAl8P4NnL7XFxvrdc/9o/H8CxcdvH\ns7flu1p3b8l+3AqAz7k1DcysAcD1AHYgz8ec/RV6D4A2AFvdPd/H/EMA3wGQGXdbPo+X0hN+F8lH\nf8zn3b9IzKwcwJMAvuXuEybpy8cxu3va3Vdi9Iq62syuPWd/3ozZzD4PoM3d375QJp/GG1eui/8E\ngAXjtuuzt+W7k2ZWBwDZ923TPJ4JzCyJ0cL/pbs/lb05r8c8xt07AGzH6PMs+TrmWwB8wcw+AvBr\nAOvN7BfI3/HGkuvi3wlgqZktMrMiAF8DsCXHY/g0tgDYmP14I0b/rs4LZmYAfgLggLv/YNyufB5z\njZnNzH5cgtHnKD5Ano7Z3R9x93p3b8DoY/ZFd/868nS8sU3DEyf3ADgE4AiA7033kx7nGd+vALQA\nGMbocxIPApiF0Sd7GgH8DkD1dI9z3Hhvxeivm+8B2JN9uyfPx/xZAO9kx7wXwPezt+ftmMeNfR1+\n/4Rf3o836k0dfiKB0hN+IoFS8YsESsUvEigVv0igVPwigVLxiwRKxS8SKBW/SKD+H20jb43tyTDd\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe72f7da6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49, 49, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGAnFFr9hJBRb/IaRUGzxG0ZC+f9G0d/WEefH3QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe72d0e9ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainImages, trainLabels, testImages, testLabels = None, None, None, None\n",
    "\n",
    "## If pickle file exists, read the file\n",
    "if os.path.isfile(root + \"/processed_images.pkl\"):\n",
    "    f = open(root + \"/processed_images.pkl\", 'rb')\n",
    "    trainImages = cPickle.load(f, encoding=\"latin1\")\n",
    "    trainLabels = cPickle.load(f, encoding=\"latin1\")\n",
    "    testImages = cPickle.load(f, encoding=\"latin1\")\n",
    "    testLabels = cPickle.load(f, encoding=\"latin1\")\n",
    "    f.close()\n",
    "## Else, read images and write to the pickle file\n",
    "else:\n",
    "    start = time.time()\n",
    "    trainImages, trainLabels = readTrafficSigns(train_dir, resize_size)\n",
    "    print(\"Training Image preprocessing finished in {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "    start = time.time()\n",
    "    testImages, testLabels = readTrafficSigns(test_dir, resize_size, False)\n",
    "    print(\"Testing Image preprocessing finished in {:.2f} seconds\".format(time.time() - start))\n",
    "    \n",
    "    f = open(root + \"/processed_images.pkl\", 'wb')\n",
    "    for obj in [trainImages, trainLabels, testImages, testLabels]:\n",
    "        cPickle.dump(obj, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "\n",
    "print(\"trainImages list length {:d}, trainLabels list length {:d}\".format(len(trainImages), len(trainLabels)))\n",
    "print(\"testImages list length {:d}, testLabels list length {:d}\".format(len(testImages), len(testLabels)))\n",
    "\n",
    "print(trainImages[42].shape)\n",
    "plt.imshow(trainImages[42])\n",
    "plt.show()\n",
    "\n",
    "print(testImages[21].shape)\n",
    "plt.imshow(trainImages[21])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intel Nervana Neon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use cpu as backend.\n",
      "Epoch 0   [Train |████████████████████|  246/246  batches, 3.57 cost, 320.94s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 1   [Train |████████████████████|  245/245  batches, 3.49 cost, 314.28s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 2   [Train |████████████████████|  245/245  batches, 3.47 cost, 316.41s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 3   [Train |████████████████████|  245/245  batches, 3.46 cost, 316.36s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 4   [Train |████████████████████|  245/245  batches, 3.44 cost, 315.60s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 5   [Train |████████████████████|  245/245  batches, 3.43 cost, 315.15s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 6   [Train |████████████████████|  245/245  batches, 3.42 cost, 315.50s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 7   [Train |████████████████████|  245/245  batches, 3.42 cost, 316.39s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 8   [Train |████████████████████|  245/245  batches, 3.41 cost, 316.64s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 9   [Train |████████████████████|  245/245  batches, 3.41 cost, 317.36s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 10  [Train |████████████████████|  245/245  batches, 3.41 cost, 317.14s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 11  [Train |████████████████████|  245/245  batches, 3.40 cost, 317.21s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 12  [Train |████████████████████|  245/245  batches, 3.40 cost, 317.77s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 13  [Train |████████████████████|  245/245  batches, 3.40 cost, 317.65s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 14  [Train |████████████████████|  245/245  batches, 3.40 cost, 318.21s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 15  [Train |████████████████████|  245/245  batches, 3.40 cost, 317.82s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 16  [Train |████████████████████|  245/245  batches, 3.40 cost, 318.24s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 17  [Train |████████████████████|  245/245  batches, 3.40 cost, 319.09s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 18  [Train |████████████████████|  246/246  batches, 3.41 cost, 320.16s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 19  [Train |████████████████████|  245/245  batches, 3.41 cost, 318.13s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 20  [Train |████████████████████|  245/245  batches, 3.40 cost, 318.38s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 21  [Train |████████████████████|  245/245  batches, 3.40 cost, 318.88s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 22  [Train |████████████████████|  245/245  batches, 3.40 cost, 318.41s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 23  [Train |████████████████████|  245/245  batches, 3.39 cost, 319.05s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 24  [Train |████████████████████|  245/245  batches, 3.39 cost, 319.32s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 25  [Train |████████████████████|  245/245  batches, 3.38 cost, 318.04s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 26  [Train |████████████████████|  245/245  batches, 3.38 cost, 318.52s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 27  [Train |████████████████████|  245/245  batches, 3.38 cost, 318.75s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 28  [Train |████████████████████|  245/245  batches, 3.38 cost, 318.72s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 29  [Train |████████████████████|  245/245  batches, 3.38 cost, 319.12s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 30  [Train |████████████████████|  245/245  batches, 3.38 cost, 319.96s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 31  [Train |████████████████████|  245/245  batches, 3.38 cost, 318.09s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 32  [Train |████████████████████|  245/245  batches, 3.38 cost, 321.56s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 33  [Train |████████████████████|  245/245  batches, 3.38 cost, 322.40s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 34  [Train |████████████████████|  245/245  batches, 3.37 cost, 322.28s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 35  [Train |████████████████████|  245/245  batches, 3.37 cost, 322.81s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 36  [Train |████████████████████|  246/246  batches, 3.38 cost, 325.06s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 37  [Train |████████████████████|  245/245  batches, 3.38 cost, 323.59s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 38  [Train |████████████████████|  245/245  batches, 3.38 cost, 323.81s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 39  [Train |████████████████████|  245/245  batches, 3.38 cost, 323.93s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 40  [Train |████████████████████|  245/245  batches, 3.37 cost, 324.22s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 41  [Train |████████████████████|  245/245  batches, 3.36 cost, 323.60s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 42  [Train |████████████████████|  245/245  batches, 3.35 cost, 323.68s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 43  [Train |████████████████████|  245/245  batches, 3.35 cost, 324.44s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 44  [Train |████████████████████|  245/245  batches, 3.34 cost, 323.93s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 45  [Train |████████████████████|  245/245  batches, 3.34 cost, 322.98s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 46  [Train |████████████████████|  245/245  batches, 3.34 cost, 322.73s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 47  [Train |████████████████████|  245/245  batches, 3.33 cost, 323.09s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 48  [Train |████████████████████|  245/245  batches, 3.33 cost, 323.46s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 49  [Train |████████████████████|  245/245  batches, 3.32 cost, 323.71s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 50  [Train |████████████████████|  245/245  batches, 3.32 cost, 322.83s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 51  [Train |████████████████████|  245/245  batches, 3.31 cost, 323.38s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 52  [Train |████████████████████|  245/245  batches, 3.30 cost, 323.00s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 53  [Train |████████████████████|  245/245  batches, 3.29 cost, 321.68s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 54  [Train |████████████████████|  246/246  batches, 3.31 cost, 323.02s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 55  [Train |████████████████████|  245/245  batches, 3.30 cost, 322.13s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 56  [Train |████████████████████|  245/245  batches, 3.30 cost, 322.93s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 57  [Train |████████████████████|  245/245  batches, 3.29 cost, 322.30s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 58  [Train |████████████████████|  245/245  batches, 3.28 cost, 323.12s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 59  [Train |████████████████████|  245/245  batches, 3.28 cost, 322.91s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Neon training finishes in 20896.62 seconds.\n",
      "Misclassification error = 88.3%. Finished in 27.81 seconds.\n",
      "Top 3 Misclassification error = 73.3%. Finished in 28.00 seconds.\n",
      "Misclassification error = 88.0% on test set. Finished in 44.30 seconds.\n",
      "Top 3 Misclassification error = 76.0% on test set. Finished in 44.36 seconds.\n",
      "\n",
      "\n",
      "Use mkl as backend.\n",
      "Epoch 0   [Train |████████████████████|  246/246  batches, 3.57 cost, 108.83s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 1   [Train |████████████████████|  245/245  batches, 3.50 cost, 101.33s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 2   [Train |████████████████████|  245/245  batches, 3.53 cost, 104.16s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 3   [Train |████████████████████|  245/245  batches, 3.53 cost, 108.83s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 4   [Train |████████████████████|  245/245  batches, 7.53 cost, 102.71s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 5   [Train |████████████████████|  245/245  batches, 9.75 cost, 99.56s]  [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 6   [Train |████████████████████|  245/245  batches, 3.51 cost, 108.41s] [CrossEntropyMulti Loss 0.00, 0.00s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7   [Train |████████████████████|  245/245  batches, 3.51 cost, 103.23s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 8   [Train |████████████████████|  245/245  batches, 3.49 cost, 100.11s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 9   [Train |████████████████████|  245/245  batches, 3.49 cost, 99.82s]  [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 10  [Train |████████████████████|  245/245  batches, 3.77 cost, 104.89s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 11  [Train |████████████████████|  245/245  batches, 6.76 cost, 102.81s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 12  [Train |████████████████████|  245/245  batches, 6.75 cost, 102.98s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 13  [Train |████████████████████|  245/245  batches, 3.60 cost, 102.75s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 14  [Train |████████████████████|  245/245  batches, 4.79 cost, 103.78s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 15  [Train |████████████████████|  245/245  batches, 3.49 cost, 104.17s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 16  [Train |████████████████████|  245/245  batches, 5.13 cost, 105.17s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 17  [Train |████████████████████|  245/245  batches, 3.49 cost, 99.27s]  [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 18  [Train |████████████████████|  246/246  batches, 3.50 cost, 93.31s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 19  [Train |████████████████████|  245/245  batches, 3.50 cost, 92.08s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 20  [Train |████████████████████|  245/245  batches, 3.50 cost, 102.36s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 21  [Train |████████████████████|  245/245  batches, 3.50 cost, 101.66s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 22  [Train |████████████████████|  245/245  batches, 3.49 cost, 92.64s]  [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 23  [Train |████████████████████|  245/245  batches, 3.86 cost, 97.99s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 24  [Train |████████████████████|  245/245  batches, 3.50 cost, 103.29s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 25  [Train |████████████████████|  245/245  batches, 3.49 cost, 101.68s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 26  [Train |████████████████████|  245/245  batches, 3.49 cost, 103.54s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 27  [Train |████████████████████|  245/245  batches, 3.49 cost, 103.03s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 28  [Train |████████████████████|  245/245  batches, 3.49 cost, 101.58s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 29  [Train |████████████████████|  245/245  batches, 3.49 cost, 101.20s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 30  [Train |████████████████████|  245/245  batches, 3.49 cost, 97.45s]  [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 31  [Train |████████████████████|  245/245  batches, 3.49 cost, 104.21s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 32  [Train |████████████████████|  245/245  batches, 3.49 cost, 94.51s]  [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 33  [Train |████████████████████|  245/245  batches, 3.60 cost, 102.85s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 34  [Train |████████████████████|  245/245  batches, 3.49 cost, 102.16s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 35  [Train |████████████████████|  245/245  batches, 3.49 cost, 100.07s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 36  [Train |████████████████████|  246/246  batches, 3.50 cost, 101.97s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 37  [Train |████████████████████|  245/245  batches, 3.50 cost, 105.87s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 38  [Train |████████████████████|  245/245  batches, 3.50 cost, 99.23s]  [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 39  [Train |████████████████████|  245/245  batches, 3.50 cost, 100.64s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 40  [Train |████████████████████|  245/245  batches, 3.49 cost, 105.28s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 41  [Train |████████████████████|  245/245  batches, 3.49 cost, 99.28s]  [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 42  [Train |████████████████████|  245/245  batches, 3.49 cost, 98.03s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 43  [Train |████████████████████|  245/245  batches, 3.49 cost, 100.58s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 44  [Train |████████████████████|  245/245  batches, 3.49 cost, 93.07s]  [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 45  [Train |████████████████████|  245/245  batches, 3.49 cost, 91.11s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 46  [Train |████████████████████|  245/245  batches, 3.49 cost, 97.26s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 47  [Train |████████████████████|  245/245  batches, 3.49 cost, 102.62s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 48  [Train |████████████████████|  245/245  batches, 3.49 cost, 100.45s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 49  [Train |████████████████████|  245/245  batches, 3.49 cost, 98.77s]  [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 50  [Train |████████████████████|  245/245  batches, 3.49 cost, 102.17s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 51  [Train |████████████████████|  245/245  batches, 3.49 cost, 103.60s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 52  [Train |████████████████████|  245/245  batches, 3.49 cost, 101.27s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 53  [Train |████████████████████|  245/245  batches, 3.49 cost, 98.49s]  [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 54  [Train |████████████████████|  246/246  batches, 3.54 cost, 100.23s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 55  [Train |████████████████████|  245/245  batches, 3.50 cost, 98.81s]  [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 56  [Train |████████████████████|  245/245  batches, 3.50 cost, 100.31s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 57  [Train |████████████████████|  245/245  batches, 3.50 cost, 89.09s]  [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 58  [Train |████████████████████|  245/245  batches, 3.50 cost, 97.33s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 59  [Train |████████████████████|  245/245  batches, 3.49 cost, 99.99s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Neon training finishes in 6686.19 seconds.\n",
      "Misclassification error = 94.3%. Finished in 9.76 seconds.\n",
      "Top 3 Misclassification error = 83.4%. Finished in 9.72 seconds.\n",
      "Misclassification error = 94.1% on test set. Finished in 15.56 seconds.\n",
      "Top 3 Misclassification error = 82.7% on test set. Finished in 16.33 seconds.\n",
      "\n",
      "\n",
      "Use gpu as backend.\n",
      "Epoch 0   [Train |████████████████████|  246/246  batches, 3.57 cost, 4.46s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 1   [Train |████████████████████|  245/245  batches, 3.49 cost, 4.43s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 2   [Train |████████████████████|  245/245  batches, 3.47 cost, 4.43s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 3   [Train |████████████████████|  245/245  batches, 3.46 cost, 4.43s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 4   [Train |████████████████████|  245/245  batches, 3.44 cost, 4.43s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 5   [Train |████████████████████|  245/245  batches, 3.43 cost, 4.44s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 6   [Train |████████████████████|  245/245  batches, 3.42 cost, 4.44s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 7   [Train |████████████████████|  245/245  batches, 3.42 cost, 4.44s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 8   [Train |████████████████████|  245/245  batches, 3.41 cost, 4.44s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 9   [Train |████████████████████|  245/245  batches, 3.41 cost, 4.45s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 10  [Train |████████████████████|  245/245  batches, 3.41 cost, 4.45s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 11  [Train |████████████████████|  245/245  batches, 3.41 cost, 4.45s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 12  [Train |████████████████████|  245/245  batches, 3.40 cost, 4.45s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 13  [Train |████████████████████|  245/245  batches, 3.40 cost, 4.45s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 14  [Train |████████████████████|  245/245  batches, 3.40 cost, 4.44s] [CrossEntropyMulti Loss 0.00, 0.00s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15  [Train |████████████████████|  245/245  batches, 3.40 cost, 4.44s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 16  [Train |████████████████████|  245/245  batches, 3.40 cost, 4.45s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 17  [Train |████████████████████|  245/245  batches, 3.40 cost, 4.45s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 18  [Train |████████████████████|  246/246  batches, 3.41 cost, 4.47s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 19  [Train |████████████████████|  245/245  batches, 3.41 cost, 4.45s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 20  [Train |████████████████████|  245/245  batches, 3.40 cost, 4.46s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 21  [Train |████████████████████|  245/245  batches, 3.40 cost, 4.45s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 22  [Train |████████████████████|  245/245  batches, 3.39 cost, 4.45s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 23  [Train |████████████████████|  245/245  batches, 3.39 cost, 4.47s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 24  [Train |████████████████████|  245/245  batches, 3.39 cost, 4.48s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 25  [Train |████████████████████|  245/245  batches, 3.39 cost, 4.48s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 26  [Train |████████████████████|  245/245  batches, 3.39 cost, 4.48s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 27  [Train |████████████████████|  245/245  batches, 3.38 cost, 4.48s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 28  [Train |████████████████████|  245/245  batches, 3.38 cost, 4.49s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 29  [Train |████████████████████|  245/245  batches, 3.38 cost, 4.48s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 30  [Train |████████████████████|  245/245  batches, 3.38 cost, 4.47s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 31  [Train |████████████████████|  245/245  batches, 3.38 cost, 4.48s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 32  [Train |████████████████████|  245/245  batches, 3.38 cost, 4.49s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 33  [Train |████████████████████|  245/245  batches, 3.38 cost, 4.48s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 34  [Train |████████████████████|  245/245  batches, 3.37 cost, 4.48s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 35  [Train |████████████████████|  245/245  batches, 3.37 cost, 4.48s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 36  [Train |████████████████████|  246/246  batches, 3.38 cost, 4.50s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 37  [Train |████████████████████|  245/245  batches, 3.38 cost, 4.48s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 38  [Train |████████████████████|  245/245  batches, 3.38 cost, 4.48s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 39  [Train |████████████████████|  245/245  batches, 3.37 cost, 4.48s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 40  [Train |████████████████████|  245/245  batches, 3.37 cost, 4.46s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 41  [Train |████████████████████|  245/245  batches, 3.36 cost, 4.46s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 42  [Train |████████████████████|  245/245  batches, 3.36 cost, 4.46s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 43  [Train |████████████████████|  245/245  batches, 3.35 cost, 4.46s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 44  [Train |████████████████████|  245/245  batches, 3.34 cost, 4.46s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 45  [Train |████████████████████|  245/245  batches, 3.34 cost, 4.47s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 46  [Train |████████████████████|  245/245  batches, 3.33 cost, 4.46s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 47  [Train |████████████████████|  245/245  batches, 3.32 cost, 4.46s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 48  [Train |████████████████████|  245/245  batches, 3.32 cost, 4.46s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 49  [Train |████████████████████|  245/245  batches, 3.32 cost, 4.46s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 50  [Train |████████████████████|  245/245  batches, 3.31 cost, 4.47s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 51  [Train |████████████████████|  245/245  batches, 3.30 cost, 4.46s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 52  [Train |████████████████████|  245/245  batches, 3.30 cost, 4.46s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 53  [Train |████████████████████|  245/245  batches, 3.29 cost, 4.47s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 54  [Train |████████████████████|  246/246  batches, 3.31 cost, 4.48s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 55  [Train |████████████████████|  245/245  batches, 3.30 cost, 4.46s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 56  [Train |████████████████████|  245/245  batches, 3.30 cost, 4.46s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 57  [Train |████████████████████|  245/245  batches, 3.30 cost, 4.46s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 58  [Train |████████████████████|  245/245  batches, 3.29 cost, 4.46s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Epoch 59  [Train |████████████████████|  245/245  batches, 3.28 cost, 4.47s] [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "Neon training finishes in 292.81 seconds.\n",
      "Misclassification error = 88.2%. Finished in 0.41 seconds.\n",
      "Top 3 Misclassification error = 73.1%. Finished in 0.41 seconds.\n",
      "Misclassification error = 87.9% on test set. Finished in 0.65 seconds.\n",
      "Top 3 Misclassification error = 76.0% on test set. Finished in 0.65 seconds.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from neon.backends import gen_backend, cleanup_backend\n",
    "from neon.initializers import Gaussian, Constant, GlorotUniform\n",
    "from neon.layers import GeneralizedCost, Affine\n",
    "from neon.layers import Conv as neon_Conv, Dropout as neon_Dropout, Pooling as neon_Pooling\n",
    "from neon.transforms import Rectlin, Softmax, CrossEntropyMulti, Misclassification, TopKMisclassification\n",
    "from neon.models import Model\n",
    "from neon.optimizers import GradientDescentMomentum as neon_SGD, ExpSchedule\n",
    "from neon.callbacks.callbacks import Callbacks, LossCallback\n",
    "from neon.data.dataiterator import ArrayIterator\n",
    "\n",
    "mlp = None\n",
    "\n",
    "# if os.path.isfile(root + \"/saved_models/neon_weights.prm\"):\n",
    "#     print(\"Model exists\")\n",
    "# else:\n",
    "\n",
    "neon_backends = [\"cpu\", \"mkl\", \"gpu\"]\n",
    "\n",
    "for b in neon_backends:\n",
    "    print(\"Use {} as backend.\".format(b))\n",
    "    \n",
    "    # Set up backend\n",
    "    # backend: 'cpu' for single, 'mkl' for multi-thread cpu, and 'gpu' for gpu\n",
    "    be = gen_backend(backend=b, batch_size=batch_size, rng_seed=542, datatype=np.float32)\n",
    "\n",
    "    # Make iterators\n",
    "    x_train, x_valid, neon_y_train, neon_y_valid = ms.train_test_split(trainImages, trainLabels, test_size=0.2, random_state=542)\n",
    "    neon_train_set = ArrayIterator(X=np.asarray([t.flatten().astype('float32')/255 for t in x_train]), y=np.asarray(neon_y_train), make_onehot=True, nclass=43, lshape=(3, resize_size[0], resize_size[1]))\n",
    "    neon_valid_set = ArrayIterator(X=np.asarray([t.flatten().astype('float32')/255 for t in x_valid]), y=np.asarray(neon_y_valid), make_onehot=True, nclass=43, lshape=(3, resize_size[0], resize_size[1]))\n",
    "\n",
    "    # Construct CNN\n",
    "    neon_gaussInit = Gaussian(loc=0.0, scale=0.01)\n",
    "    layers = []\n",
    "    layers.append(neon_Conv((5, 5, 64), strides=2, init=neon_gaussInit, bias=Constant(0.0), activation=Rectlin(), name=\"neon_conv1\"))\n",
    "    layers.append(neon_Pooling(2, op=\"max\", strides=2, name=\"neon_pool1\"))\n",
    "    layers.append(neon_Conv((3, 3, 512), strides=1, padding=1, init=neon_gaussInit, bias=Constant(0.0), activation=Rectlin(), name=\"neon_conv2\"))\n",
    "    layers.append(neon_Pooling(2, op=\"max\", strides=2, name=\"neon_pool2\"))\n",
    "    layers.append(neon_Pooling(5, op=\"avg\", name=\"neon_global_pool\"))\n",
    "    layers.append(Affine(nout=4096, init=neon_gaussInit, bias=Constant(0.0), activation=Rectlin(), name=\"neon_fc1\"))\n",
    "    layers.append(neon_Dropout(keep=0.5, name=\"neon_drop_out\"))\n",
    "    layers.append(Affine(nout=43, init=neon_gaussInit, bias=Constant(0.0), activation=Softmax(), name=\"neon_fc2\"))\n",
    "\n",
    "    # Initialize model object\n",
    "    mlp = Model(layers=layers)\n",
    "\n",
    "    # Costs\n",
    "    neon_cost = GeneralizedCost(costfunc=CrossEntropyMulti())\n",
    "\n",
    "    # Model summary\n",
    "    mlp.initialize(neon_train_set, neon_cost)\n",
    "    # print(mlp)\n",
    "\n",
    "    # Learning rules\n",
    "    neon_optimizer = neon_SGD(0.01, momentum_coef=0.9, gradient_clip_value=5, schedule=ExpSchedule(0.2))\n",
    "\n",
    "    # Callbacks: validate on validation set\n",
    "    callbacks = Callbacks(mlp, eval_set=neon_valid_set, output_file=root+\"/callback_data_{}.h5\".format(b))\n",
    "    callbacks.add_callback(LossCallback(eval_set=neon_valid_set, epoch_freq=1))\n",
    "\n",
    "    # Fit\n",
    "    start = time.time()\n",
    "    mlp.fit(neon_train_set, optimizer=neon_optimizer, num_epochs=epoch_num, cost=neon_cost, callbacks=callbacks)\n",
    "    print(\"Neon training finishes in {:.2f} seconds.\".format(time.time() - start))\n",
    "\n",
    "    # Result\n",
    "    results = mlp.get_outputs(neon_valid_set)\n",
    "\n",
    "    # Print error on validation set\n",
    "    start = time.time()\n",
    "    neon_error_mis = mlp.eval(neon_valid_set, metric=Misclassification())*100\n",
    "    print('Misclassification error = {:.1f}%. Finished in {:.2f} seconds.'.format(neon_error_mis[0], time.time() - start))\n",
    "\n",
    "    start = time.time()\n",
    "    neon_error_top3 = mlp.eval(neon_valid_set, metric=TopKMisclassification(3))*100\n",
    "    print('Top 3 Misclassification error = {:.1f}%. Finished in {:.2f} seconds.'.format(neon_error_top3[2], time.time() - start))\n",
    "\n",
    "    mlp.save_params(root + \"/saved_models/neon_weights_{}.prm\".format(b))\n",
    "\n",
    "    # Print error on test set\n",
    "    neon_test_set = ArrayIterator(X=np.asarray([t.flatten().astype('float32')/255 for t in testImages]), y=np.asarray(testLabels), nclass=43, lshape=(3, resize_size[0], resize_size[1]))\n",
    "\n",
    "    start = time.time()\n",
    "    neon_error_mis_t = mlp.eval(neon_test_set, metric=Misclassification())*100\n",
    "    print('Misclassification error = {:.1f}% on test set. Finished in {:.2f} seconds.'.format(neon_error_mis_t[0], time.time() - start))\n",
    "\n",
    "    start = time.time()\n",
    "    neon_error_top3_t = mlp.eval(neon_test_set, metric=TopKMisclassification(3))*100\n",
    "    print('Top 3 Misclassification error = {:.1f}% on test set. Finished in {:.2f} seconds.'.format(neon_error_top3_t[2], time.time() - start))\n",
    "\n",
    "    cleanup_backend()\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras with different multiple backends (Tensorflow, Theano, CNTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D as keras_Conv, MaxPooling2D as keras_MaxPooling, GlobalAveragePooling2D as keras_AveragePooling\n",
    "from keras.layers import Dropout as keras_Dropout, Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import image\n",
    "from keras.initializers import RandomNormal, Constant as keras_Constant\n",
    "from keras.optimizers import SGD as keras_SGD\n",
    "from keras.callbacks import ModelCheckpoint, Callback as keras_callback\n",
    "from sklearn import model_selection as ms\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.layers.convolutional import ZeroPadding2D\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LossHistory(keras_callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to dynamically change keras backend\n",
    "from importlib import reload\n",
    "def set_keras_backend(backend):\n",
    "    if K.backend() != backend:\n",
    "        os.environ['KERAS_BACKEND'] = backend\n",
    "        reload(K)\n",
    "        assert K.backend() == backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(os.environ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sys import platform\n",
    "backends = [\"theano\", \"tensorflow\"]\n",
    "if platform != \"darwin\":\n",
    "    backends.append(\"cntk\")\n",
    "    \n",
    "for b in backends:\n",
    "    set_keras_backend(b)\n",
    "\n",
    "    # Load and process images\n",
    "    enc = OneHotEncoder(sparse=False)\n",
    "    x_train, x_valid, y_train, y_valid = ms.train_test_split(trainImages, trainLabels, test_size=0.2, random_state=542)\n",
    "    keras_train_x = np.vstack([np.expand_dims(image.img_to_array(x), axis=0).astype('float32')/255 for x in x_train])\n",
    "    keras_valid_x = np.vstack([np.expand_dims(image.img_to_array(x), axis=0).astype('float32')/255 for x in x_valid])\n",
    "    keras_test_x = np.vstack([np.expand_dims(image.img_to_array(x), axis=0).astype('float32')/255 for x in testImages])\n",
    "    keras_train_y = to_categorical(y_train, 43)\n",
    "    keras_valid_y = to_categorical(y_valid, 43)\n",
    "    keras_test_y = to_categorical(testLabels, 43)\n",
    "\n",
    "    # Build model\n",
    "    keras_gaussInit = RandomNormal(mean=0.0, stddev=0.01, seed=542)\n",
    "    layer_name_prefix = b+\"_\"\n",
    "\n",
    "    keras_model = Sequential()\n",
    "    keras_model.add(keras_Conv(64, (5, 5), kernel_initializer=keras_gaussInit, strides=(2, 2), bias_initializer=keras_Constant(0.0), activation=\"relu\", input_shape=(resize_size[0], resize_size[1], 3), name=layer_name_prefix+\"conv1\"))\n",
    "    keras_model.add(keras_MaxPooling(pool_size=(2, 2), name=layer_name_prefix+\"pool1\"))\n",
    "    # keras_model.add(ZeroPadding2D(padding=(1, 1), name=layer_name_prefix+\"zero_padding\"))\n",
    "    # keras_model.add(keras_Conv(256, (3, 3), kernel_initializer=keras_gaussInit, strides=(1, 1), bias_initializer=keras_Constant(0.0), activation=\"relu\", name=layer_name_prefix+\"conv2\"))\n",
    "    keras_model.add(keras_Conv(256, (3, 3), kernel_initializer=keras_gaussInit, strides=(1, 1), padding=\"same\", bias_initializer=keras_Constant(0.0), activation=\"relu\", name=layer_name_prefix+\"conv2\"))\n",
    "    keras_model.add(keras_MaxPooling(pool_size=(2, 2), name=layer_name_prefix+\"pool2\"))\n",
    "    keras_model.add(keras_AveragePooling(name=layer_name_prefix+\"global_pool\"))\n",
    "#     keras_model.add(Flatten(name=layer_name_prefix+\"flatten\")) # An extra layer to flatten the previous layer in order to connect to fully connected layer\n",
    "    keras_model.add(Dense(4096, kernel_initializer=keras_gaussInit, bias_initializer=keras_Constant(0.0), activation=\"relu\", name=layer_name_prefix+\"fc1\"))\n",
    "    keras_model.add(keras_Dropout(0.5, name=layer_name_prefix+\"drop_out\"))\n",
    "    keras_model.add(Dense(43, kernel_initializer=keras_gaussInit, bias_initializer=keras_Constant(0.0), activation=\"softmax\", name=layer_name_prefix+\"fc2\"))\n",
    "    keras_model.summary()\n",
    "\n",
    "    keras_optimizer = keras_SGD(lr=0.01, decay=1.6e-8, momentum=0.9)\n",
    "    keras_cost = \"categorical_crossentropy\"\n",
    "    keras_model.compile(loss=keras_cost, optimizer=keras_optimizer, metrics=[\"acc\"])\n",
    "\n",
    "    checkpointer = ModelCheckpoint(filepath=root+\"/saved_models/keras_\"+b+\"_weights.hdf5\",\n",
    "                                       verbose=1, save_best_only=True)\n",
    "\n",
    "    start = time.time()\n",
    "    keras_model.fit(keras_train_x, keras_train_y,\n",
    "                  validation_data=(keras_valid_x, keras_valid_y),\n",
    "                  epochs=epoch_num, batch_size=batch_size, callbacks=[checkpointer, LossHistory()], verbose=1, shuffle=True)\n",
    "    print(\"{} training finishes in {:.2f} seconds.\".format(b, time.time() - start))\n",
    "\n",
    "    keras_model.load_weights(root+\"/saved_models/keras_\"+b+\"_weights.hdf5\")\n",
    "    keras_predictions = [np.argmax(keras_model.predict(np.expand_dims(feature, axis=0))) for feature in keras_test_x]\n",
    "\n",
    "    # report test accuracy\n",
    "    keras_test_accuracy = 100*np.sum(np.array(keras_predictions)==np.argmax(keras_test_y, axis=1))/len(keras_predictions)\n",
    "    print('{} test accuracy: {:.1f}%'.format(b, keras_test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as utils\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        # Build model\n",
    "        self.conv = torch.nn.Sequential()\n",
    "        self.conv.add_module(\"torch_conv1\", torch.nn.Conv2d(3, 64, kernel_size=(5, 5), stride=2))\n",
    "        self.conv.add_module(\"torch_pool1\", torch.nn.MaxPool2d(kernel_size=2))\n",
    "        self.conv.add_module(\"torch_relu1\", torch.nn.ReLU())\n",
    "        self.conv.add_module(\"torch_conv2\", torch.nn.Conv2d(64, 256, kernel_size=(3, 3), stride=1, padding=1))\n",
    "        self.conv.add_module(\"torch_pool2\", torch.nn.MaxPool2d(kernel_size=2))\n",
    "        self.conv.add_module(\"torch_relu2\", torch.nn.ReLU())\n",
    "        self.conv.add_module(\"torch_global_pool\", torch.nn.AvgPool2d(kernel_size=5))\n",
    "        \n",
    "        self.csf = torch.nn.Sequential()\n",
    "        self.csf.add_module(\"torch_fc1\", torch.nn.Linear(256, 4096))\n",
    "        self.csf.add_module(\"torch_relu3\", torch.nn.ReLU())\n",
    "        self.csf.add_module(\"torch_dropout1\", torch.nn.Dropout(0.5))\n",
    "        self.csf.add_module(\"torch_fc2\", torch.nn.Linear(4096, 43))\n",
    "        \n",
    "        # Initialize conv layers and fc layers\n",
    "        torch.nn.init.normal(self.conv.state_dict()[\"torch_conv1.weight\"], mean=0, std=0.01)\n",
    "        torch.nn.init.constant(self.conv.state_dict()[\"torch_conv1.bias\"], 0.0)\n",
    "        torch.nn.init.normal(self.conv.state_dict()[\"torch_conv2.weight\"], mean=0, std=0.01)\n",
    "        torch.nn.init.constant(self.conv.state_dict()[\"torch_conv2.bias\"], 0.0)\n",
    "        torch.nn.init.normal(self.csf.state_dict()[\"torch_fc1.weight\"], mean=0, std=0.01)\n",
    "        torch.nn.init.constant(self.csf.state_dict()[\"torch_fc1.bias\"], 0.0)\n",
    "        torch.nn.init.normal(self.csf.state_dict()[\"torch_fc2.weight\"], mean=0, std=0.01)\n",
    "        torch.nn.init.constant(self.csf.state_dict()[\"torch_fc2.bias\"], 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv.forward(x)\n",
    "        x = x.view(-1, 256)\n",
    "        return self.csf.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = ms.train_test_split(trainImages, trainLabels, test_size=0.2, random_state=542)\n",
    "\n",
    "torch_train_x = torch.stack([torch.Tensor(i.swapaxes(0,2).astype(\"float32\")/255) for i in x_train])\n",
    "torch_train_y = torch.LongTensor(y_train)\n",
    "torch_valid_x = torch.stack([torch.Tensor(i.swapaxes(0,2).astype(\"float32\")/255) for i in x_valid])\n",
    "torch_valid_y = torch.LongTensor(y_valid)\n",
    "torch_test_x = torch.stack([torch.Tensor(i.swapaxes(0,2).astype(\"float32\")/255) for i in testImages])\n",
    "torch_test_y = torch.LongTensor(testLabels)\n",
    "\n",
    "torch_tensor_train_set = utils.TensorDataset(torch_train_x, torch_train_y)\n",
    "torch_train_set = utils.DataLoader(torch_tensor_train_set, batch_size=batch_size, shuffle=True)\n",
    "torch_tensor_valid_set = utils.TensorDataset(torch_valid_x, torch_valid_y)\n",
    "torch_valid_set = utils.DataLoader(torch_tensor_valid_set, batch_size=batch_size, shuffle=True)\n",
    "torch_tensor_test_set = utils.TensorDataset(torch_test_x, torch_test_y)\n",
    "torch_test_set = utils.DataLoader(torch_tensor_test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "torch_model = ConvNet()\n",
    "optimizer = optim.SGD(torch_model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    torch_model.train()\n",
    "    for batch_idx, (data, target) in enumerate(torch_train_set):\n",
    "#         if args.cuda:\n",
    "#             data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = torch_model(data)\n",
    "        cost = torch.nn.CrossEntropyLoss(size_average=True)\n",
    "        loss = cost(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(torch_train_set.dataset),\n",
    "                100. * batch_idx / len(torch_train_set), loss.data[0]))\n",
    "def test():\n",
    "    torch_model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in torch_test_set:\n",
    "#         if args.cuda:\n",
    "#             data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = torch_model(data)\n",
    "        cost = torch.nn.CrossEntropyLoss(size_average=False)\n",
    "        test_loss += cost(output, target).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(torch_test_set.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(torch_test_set.dataset),\n",
    "        100. * correct / len(torch_test_set.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, epoch_num + 1):\n",
    "    train(epoch)\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
