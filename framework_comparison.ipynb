{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Comparison of Deep Learning Frameworks\n",
    "\n",
    "## Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv, time, os.path\n",
    "from six.moves import cPickle\n",
    "from sklearn import model_selection as ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for reading the images\n",
    "# arguments: path to the traffic sign data, for example './GTSRB/Training'\n",
    "# returns: list of images, list of corresponding labels \n",
    "def readTrafficSigns(rootpath, size, training=True):\n",
    "    '''Reads traffic sign data for German Traffic Sign Recognition Benchmark.\n",
    "\n",
    "    Arguments: path to the traffic sign data, for example './GTSRB/Training'\n",
    "    Returns:   list of images, list of corresponding labels'''\n",
    "    images = [] # images\n",
    "    labels = [] # corresponding labels\n",
    "    # loop over all 43 classes\n",
    "    if training:\n",
    "        for c in range(0,43):\n",
    "            prefix = rootpath + '/' + format(c, '05d') + '/' # subdirectory for class\n",
    "            gtFile = open(prefix + 'GT-'+ format(c, '05d') + '.csv') # annotations file\n",
    "            gtReader = csv.reader(gtFile, delimiter=';') # csv parser for annotations file\n",
    "            next(gtReader) # skip header\n",
    "            # loop over all images in current annotations file\n",
    "            for row in gtReader:\n",
    "#                 image = Image.open(prefix + row[0]).convert('L') # Load an image and convert to grayscale\n",
    "                image = Image.open(prefix + row[0])\n",
    "                box = (int(row[3]), int(row[4]), int(row[5]), int(row[6])) # Specify ROI box\n",
    "                image = image.crop(box) # Crop the ROI\n",
    "                image = image.resize(size) # Resize images\n",
    "                images.append(np.asarray(image).astype('uint8')) # the 1th column is the filename, while 3,4,5,6 are the vertices of ROI\n",
    "                labels.append(int(row[7])) # the 8th column is the label\n",
    "            gtFile.close()\n",
    "    else:\n",
    "        gtFile = open(rootpath + \"/../../GT-final_test.csv\") # annotations file\n",
    "        gtReader = csv.reader(gtFile, delimiter=';') # csv parser for annotations file\n",
    "        next(gtReader) # skip header\n",
    "        # loop over all images in current annotations file\n",
    "        for row in gtReader:\n",
    "#             image = Image.open(rootpath + '/' + row[0]).convert('L') # Load an image and convert to grayscale\n",
    "            image = Image.open(rootpath + '/' + row[0]) # Color version\n",
    "            box = (int(row[3]), int(row[4]), int(row[5]), int(row[6])) # Specify ROI box\n",
    "            image = image.crop(box) # Crop the ROI\n",
    "            image = image.resize(size) # Resize images\n",
    "            images.append(np.asarray(image).astype('uint8')) # the 1th column is the filename, while 3,4,5,6 are the vertices of ROI\n",
    "            labels.append(int(row[7])) # the 8th column is the label\n",
    "        gtFile.close()\n",
    "        \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zhongyilin/Desktop/GTSRB/try\n"
     ]
    }
   ],
   "source": [
    "from sys import platform\n",
    "global root\n",
    "global epoch_num\n",
    "if platform == \"darwin\":\n",
    "    root = \"/Users/moderato/Downloads/GTSRB/try\"\n",
    "else:\n",
    "    root = \"/home/zhongyilin/Desktop/GTSRB/try\"\n",
    "print(root)\n",
    "train_dir = root + \"/Final_Training/Images\"\n",
    "test_dir = root + \"/Final_Test/Images\"\n",
    "resize_size = (49, 49)\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainImages list length 39209, trainLabels list length 39209\n",
      "testImages list length 12630, testLabels list length 12630\n",
      "(49, 49, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0efcd41ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49, 49, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0eb7c020b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainImages, trainLabels, testImages, testLabels = None, None, None, None\n",
    "\n",
    "## If pickle file exists, read the file\n",
    "if os.path.isfile(root + \"/processed_images.pkl\"):\n",
    "    f = open(root + \"/processed_images.pkl\", 'rb')\n",
    "    trainImages = cPickle.load(f, encoding=\"latin1\")\n",
    "    trainLabels = cPickle.load(f, encoding=\"latin1\")\n",
    "    testImages = cPickle.load(f, encoding=\"latin1\")\n",
    "    testLabels = cPickle.load(f, encoding=\"latin1\")\n",
    "    f.close()\n",
    "## Else, read images and write to the pickle file\n",
    "else:\n",
    "    start = time.time()\n",
    "    trainImages, trainLabels = readTrafficSigns(train_dir, resize_size)\n",
    "    print(\"Training Image preprocessing finished in {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "    start = time.time()\n",
    "    testImages, testLabels = readTrafficSigns(test_dir, resize_size, False)\n",
    "    print(\"Testing Image preprocessing finished in {:.2f} seconds\".format(time.time() - start))\n",
    "    \n",
    "    f = open(root + \"/processed_images.pkl\", 'wb')\n",
    "    for obj in [trainImages, trainLabels, testImages, testLabels]:\n",
    "        cPickle.dump(obj, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "\n",
    "print(\"trainImages list length {:d}, trainLabels list length {:d}\".format(len(trainImages), len(trainLabels)))\n",
    "print(\"testImages list length {:d}, testLabels list length {:d}\".format(len(testImages), len(testLabels)))\n",
    "\n",
    "print(trainImages[42].shape)\n",
    "plt.imshow(trainImages[42])\n",
    "plt.show()\n",
    "\n",
    "print(testImages[21].shape)\n",
    "plt.imshow(trainImages[21])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intel Nervana Neon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neon.backends import gen_backend, cleanup_backend\n",
    "from neon.initializers import Gaussian, Constant, GlorotUniform\n",
    "from neon.layers import GeneralizedCost, Affine\n",
    "from neon.layers import Conv as neon_Conv, Dropout as neon_Dropout, Pooling as neon_Pooling\n",
    "from neon.transforms import Rectlin, Softmax, CrossEntropyMulti, Misclassification, TopKMisclassification\n",
    "from neon.models import Model\n",
    "from neon.optimizers import GradientDescentMomentum as neon_SGD, ExpSchedule\n",
    "from neon.callbacks.callbacks import Callbacks, Callback, LossCallback\n",
    "from neon.data.dataiterator import ArrayIterator\n",
    "from timeit import default_timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfCallback(Callback):\n",
    "    def __init__(self, epoch_freq, minibatch_freq):\n",
    "        super(Callback, self).__init__()\n",
    "        self.epoch_freq = epoch_freq\n",
    "        self.minibatch_freq = minibatch_freq\n",
    "#         self.loss = self.be.zeros((1, 1), dtype=np.float32)\n",
    "#         self.fprop_start = None\n",
    "#         self.fprop_end = None\n",
    "#         self.bprop_end = None\n",
    "        \n",
    "    def on_train_begin(self, callback_data, model, epochs):\n",
    "        print(\"train_begin\")\n",
    "#         callback_data.create_dataset(\"cost/loss\", (epochs // self.epoch_freq,))\n",
    "#         callback_data.create_dataset(\"time/loss\", (epochs // self.epoch_freq,))\n",
    "#         callback_data[\"cost/loss\"].attrs['time_markers'] = 'epoch_freq'\n",
    "#         callback_data[\"cost/loss\"].attrs['epoch_freq'] = self.epoch_freq\n",
    "        \n",
    "#         self.fprop_start = model.be.init_mark()\n",
    "#         self.fprop_end = model.be.init_mark()\n",
    "#         self.bprop_end = model.be.init_mark()\n",
    "\n",
    "    def on_train_end(self, callback_data, model):\n",
    "        print(\"train_end\")\n",
    "        \n",
    "    def on_minibatch_begin(self, callback_data, model, epoch, minibatch):\n",
    "#         self.be.record_mark(fprop_start)\n",
    "        print(\"minibatch_begin\")\n",
    "\n",
    "    def on_minibatch_end(self, callback_data, model, epoch, minibatch):\n",
    "        print(\"minibatch_end\")\n",
    "        \n",
    "    def on_epoch_begin(self, callback_data, model, epoch):\n",
    "        print(\"epoch_begin\")\n",
    "        \n",
    "    def on_epoch_end(self, callback_data, model, epoch):\n",
    "        print(\"epoch_end\")\n",
    "#         start_loss = default_timer()\n",
    "#         nprocessed = 0\n",
    "#         self.loss[:] = 0\n",
    "#         self.eval_set.reset()\n",
    "#         for x, t in self.eval_set:\n",
    "#             x = model.fprop(x, inference=True)\n",
    "#             bsz = min(self.eval_set.ndata - nprocessed, self.be.bsz)\n",
    "#             model.cost.get_cost(x, t)\n",
    "#             nsteps = x.shape[1] // self.be.bsz if not isinstance(x, list) else \\\n",
    "#                 x[0].shape[1] // self.be.bsz\n",
    "#             costbuf = model.cost.outputs[:, :bsz * nsteps]\n",
    "#             nprocessed += bsz\n",
    "#             self.loss[:] = self.loss + self.be.sum(costbuf, axis=1) / nsteps\n",
    "#             mean_cost = float(self.loss.get() / nprocessed)\n",
    "#         callback_data[\"time/loss\"][epoch // self.epoch_freq] = (default_timer() - start_loss)\n",
    "#         callback_data[\"cost/loss\"][epoch // self.epoch_freq] = mean_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_begin\n",
      "epoch_begin\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |                    |    3/246  batches, 3.76 cost, 0.13s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |                    |    5/246  batches, 3.76 cost, 0.24s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |                    |    7/246  batches, 3.76 cost, 0.34s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |                    |    9/246  batches, 3.76 cost, 0.44s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |                    |   11/246  batches, 3.76 cost, 0.55s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |█                   |   13/246  batches, 3.76 cost, 0.65s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |█                   |   15/246  batches, 3.75 cost, 0.75s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |█                   |   17/246  batches, 3.75 cost, 0.86s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |█                   |   19/246  batches, 3.75 cost, 0.96s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |█                   |   21/246  batches, 3.75 cost, 1.06s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |█                   |   23/246  batches, 3.75 cost, 1.17s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |██                  |   25/246  batches, 3.74 cost, 1.27s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |██                  |   27/246  batches, 3.74 cost, 1.37s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |██                  |   29/246  batches, 3.74 cost, 1.47s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |██                  |   31/246  batches, 3.74 cost, 1.58s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |██                  |   33/246  batches, 3.73 cost, 1.68s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |██                  |   35/246  batches, 3.73 cost, 1.79s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |███                 |   37/246  batches, 3.73 cost, 1.89s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |███                 |   39/246  batches, 3.73 cost, 2.00s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |███                 |   41/246  batches, 3.72 cost, 2.10s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |███                 |   43/246  batches, 3.72 cost, 2.20s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |███                 |   45/246  batches, 3.71 cost, 2.30s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |███                 |   47/246  batches, 3.71 cost, 2.41s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |███                 |   49/246  batches, 3.71 cost, 2.51s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |████                |   51/246  batches, 3.71 cost, 2.61s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |████                |   53/246  batches, 3.70 cost, 2.72s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |████                |   55/246  batches, 3.70 cost, 2.82s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |████                |   57/246  batches, 3.70 cost, 2.92s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |████                |   59/246  batches, 3.69 cost, 3.02s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |████                |   61/246  batches, 3.69 cost, 3.13s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |█████               |   63/246  batches, 3.68 cost, 3.23s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |█████               |   65/246  batches, 3.68 cost, 3.33s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |█████               |   67/246  batches, 3.68 cost, 3.44s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |█████               |   69/246  batches, 3.67 cost, 3.54s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |█████               |   71/246  batches, 3.66 cost, 3.64s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |█████               |   73/246  batches, 3.65 cost, 3.75s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |██████              |   75/246  batches, 3.64 cost, 3.85s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |██████              |   77/246  batches, 3.63 cost, 3.95s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |██████              |   79/246  batches, 3.62 cost, 4.06s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |██████              |   81/246  batches, 3.61 cost, 4.16s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |██████              |   83/246  batches, 3.59 cost, 4.26s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |██████              |   85/246  batches, 3.57 cost, 4.37s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |███████             |   87/246  batches, 3.56 cost, 4.47s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |███████             |   89/246  batches, 3.55 cost, 4.57s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |███████             |   91/246  batches, 3.57 cost, 4.68s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |███████             |   93/246  batches, 3.57 cost, 4.78s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |███████             |   95/246  batches, 3.56 cost, 4.88s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |███████             |   97/246  batches, 3.56 cost, 4.98s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |████████            |   99/246  batches, 3.55 cost, 5.08s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |████████            |  101/246  batches, 3.53 cost, 5.18s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |████████            |  103/246  batches, 3.53 cost, 5.29s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |████████            |  105/246  batches, 3.52 cost, 5.39s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |████████            |  107/246  batches, 3.50 cost, 5.49s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |████████            |  109/246  batches, 3.50 cost, 5.60s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |█████████           |  111/246  batches, 3.50 cost, 5.70s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |█████████           |  113/246  batches, 3.49 cost, 5.80s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |█████████           |  115/246  batches, 3.50 cost, 5.91s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |█████████           |  117/246  batches, 3.50 cost, 6.01s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |█████████           |  119/246  batches, 3.50 cost, 6.11s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0   [Train |█████████           |  121/246  batches, 3.50 cost, 6.22s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |██████████          |  123/246  batches, 3.51 cost, 6.32s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |██████████          |  125/246  batches, 3.51 cost, 6.42s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |██████████          |  127/246  batches, 3.53 cost, 6.53s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |██████████          |  129/246  batches, 3.54 cost, 6.63s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |██████████          |  132/246  batches, 3.52 cost, 6.78s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |██████████          |  134/246  batches, 3.52 cost, 6.88s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |███████████         |  136/246  batches, 3.51 cost, 6.98s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |███████████         |  138/246  batches, 3.50 cost, 7.09s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |███████████         |  140/246  batches, 3.50 cost, 7.19s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |███████████         |  142/246  batches, 3.51 cost, 7.29s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |███████████         |  144/246  batches, 3.51 cost, 7.40s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |███████████         |  146/246  batches, 3.51 cost, 7.50s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |████████████        |  148/246  batches, 3.52 cost, 7.60s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |████████████        |  151/246  batches, 3.50 cost, 7.75s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |████████████        |  153/246  batches, 3.51 cost, 7.85s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |████████████        |  156/246  batches, 3.50 cost, 8.00s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |████████████        |  158/246  batches, 3.50 cost, 8.10s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |█████████████       |  161/246  batches, 3.50 cost, 8.25s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |█████████████       |  163/246  batches, 3.50 cost, 8.35s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |█████████████       |  165/246  batches, 3.50 cost, 8.45s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |█████████████       |  168/246  batches, 3.50 cost, 8.60s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |█████████████       |  171/246  batches, 3.50 cost, 8.75s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |██████████████      |  173/246  batches, 3.49 cost, 8.85s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |██████████████      |  176/246  batches, 3.50 cost, 9.00s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |██████████████      |  178/246  batches, 3.48 cost, 9.10s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |██████████████      |  181/246  batches, 3.49 cost, 9.25s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |██████████████      |  184/246  batches, 3.48 cost, 9.40s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |███████████████     |  187/246  batches, 3.49 cost, 9.55s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |███████████████     |  190/246  batches, 3.48 cost, 9.70s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |███████████████     |  192/246  batches, 3.46 cost, 9.80s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |███████████████     |  194/246  batches, 3.47 cost, 9.90s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |███████████████     |  196/246  batches, 3.47 cost, 10.01s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |████████████████    |  198/246  batches, 3.46 cost, 10.11s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |████████████████    |  200/246  batches, 3.46 cost, 10.21s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |████████████████    |  202/246  batches, 3.46 cost, 10.32s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |████████████████    |  204/246  batches, 3.46 cost, 10.42s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |████████████████    |  206/246  batches, 3.45 cost, 10.52s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |████████████████    |  208/246  batches, 3.47 cost, 10.62s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |█████████████████   |  210/246  batches, 3.46 cost, 10.73s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |█████████████████   |  212/246  batches, 3.47 cost, 10.83s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |█████████████████   |  214/246  batches, 3.45 cost, 10.93s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |█████████████████   |  216/246  batches, 3.45 cost, 11.04s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |█████████████████   |  218/246  batches, 3.46 cost, 11.14s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |█████████████████   |  220/246  batches, 3.48 cost, 11.25s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |██████████████████  |  222/246  batches, 3.49 cost, 11.35s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |██████████████████  |  224/246  batches, 3.51 cost, 11.45s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |██████████████████  |  226/246  batches, 3.50 cost, 11.56s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |██████████████████  |  228/246  batches, 3.48 cost, 11.66s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |██████████████████  |  230/246  batches, 3.47 cost, 11.76s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |██████████████████  |  232/246  batches, 3.48 cost, 11.87s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |███████████████████ |  234/246  batches, 3.47 cost, 11.97s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |███████████████████ |  236/246  batches, 3.48 cost, 12.07s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |███████████████████ |  238/246  batches, 3.50 cost, 12.17s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |███████████████████ |  240/246  batches, 3.49 cost, 12.28s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |███████████████████ |  242/246  batches, 3.48 cost, 12.38s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |███████████████████ |  244/246  batches, 3.50 cost, 12.48s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 0   [Train |████████████████████|  246/246  batches, 3.49 cost, 12.59s]minibatch_end\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "epoch_end\n",
      "epoch_begin\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |                    |    3/245  batches, 3.48 cost, 0.12s] minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |                    |    5/245  batches, 3.48 cost, 0.22s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |                    |    7/245  batches, 3.48 cost, 0.32s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |                    |    9/245  batches, 3.47 cost, 0.42s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |                    |   11/245  batches, 3.47 cost, 0.52s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |█                   |   13/245  batches, 3.46 cost, 0.63s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |█                   |   15/245  batches, 3.46 cost, 0.73s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |█                   |   17/245  batches, 3.46 cost, 0.83s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |█                   |   19/245  batches, 3.46 cost, 0.93s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |█                   |   21/245  batches, 3.46 cost, 1.04s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |█                   |   23/245  batches, 3.48 cost, 1.14s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |██                  |   25/245  batches, 3.48 cost, 1.24s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |██                  |   27/245  batches, 3.46 cost, 1.35s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |██                  |   29/245  batches, 3.48 cost, 1.45s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |██                  |   31/245  batches, 3.48 cost, 1.55s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |██                  |   33/245  batches, 3.48 cost, 1.65s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |██                  |   35/245  batches, 3.47 cost, 1.76s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |███                 |   37/245  batches, 3.49 cost, 1.86s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |███                 |   39/245  batches, 3.46 cost, 1.97s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |███                 |   41/245  batches, 3.46 cost, 2.07s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |███                 |   43/245  batches, 3.44 cost, 2.17s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |███                 |   45/245  batches, 3.43 cost, 2.27s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |███                 |   47/245  batches, 3.42 cost, 2.38s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |████                |   49/245  batches, 3.43 cost, 2.48s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |████                |   51/245  batches, 3.45 cost, 2.58s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |████                |   53/245  batches, 3.45 cost, 2.68s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |████                |   55/245  batches, 3.45 cost, 2.79s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |████                |   57/245  batches, 3.45 cost, 2.89s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |████                |   59/245  batches, 3.47 cost, 2.99s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |████                |   61/245  batches, 3.46 cost, 3.10s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |█████               |   63/245  batches, 3.46 cost, 3.20s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |█████               |   65/245  batches, 3.47 cost, 3.30s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |█████               |   67/245  batches, 3.47 cost, 3.41s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |█████               |   69/245  batches, 3.44 cost, 3.51s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |█████               |   71/245  batches, 3.43 cost, 3.61s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |█████               |   73/245  batches, 3.42 cost, 3.71s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |██████              |   75/245  batches, 3.44 cost, 3.82s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |██████              |   77/245  batches, 3.44 cost, 3.92s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |██████              |   79/245  batches, 3.44 cost, 4.03s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |██████              |   81/245  batches, 3.44 cost, 4.13s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |██████              |   83/245  batches, 3.43 cost, 4.23s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |██████              |   85/245  batches, 3.43 cost, 4.34s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |███████             |   87/245  batches, 3.44 cost, 4.44s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |███████             |   89/245  batches, 3.45 cost, 4.54s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |███████             |   91/245  batches, 3.46 cost, 4.64s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |███████             |   93/245  batches, 3.47 cost, 4.75s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |███████             |   95/245  batches, 3.47 cost, 4.85s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |███████             |   97/245  batches, 3.46 cost, 4.96s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |████████            |   99/245  batches, 3.45 cost, 5.06s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |████████            |  101/245  batches, 3.45 cost, 5.16s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |████████            |  103/245  batches, 3.44 cost, 5.27s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |████████            |  105/245  batches, 3.43 cost, 5.37s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |████████            |  107/245  batches, 3.40 cost, 5.47s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |████████            |  109/245  batches, 3.40 cost, 5.58s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |█████████           |  111/245  batches, 3.40 cost, 5.68s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |█████████           |  113/245  batches, 3.39 cost, 5.78s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |█████████           |  115/245  batches, 3.38 cost, 5.89s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |█████████           |  117/245  batches, 3.39 cost, 5.99s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |█████████           |  119/245  batches, 3.39 cost, 6.09s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1   [Train |█████████           |  121/245  batches, 3.39 cost, 6.20s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |██████████          |  123/245  batches, 3.39 cost, 6.30s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |██████████          |  125/245  batches, 3.41 cost, 6.40s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |██████████          |  127/245  batches, 3.41 cost, 6.50s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |██████████          |  129/245  batches, 3.41 cost, 6.61s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |██████████          |  132/245  batches, 3.39 cost, 6.76s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |██████████          |  134/245  batches, 3.39 cost, 6.86s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |███████████         |  136/245  batches, 3.36 cost, 6.96s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |███████████         |  138/245  batches, 3.33 cost, 7.06s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |███████████         |  141/245  batches, 3.35 cost, 7.21s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |███████████         |  143/245  batches, 3.34 cost, 7.31s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |███████████         |  146/245  batches, 3.34 cost, 7.46s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |████████████        |  148/245  batches, 3.35 cost, 7.56s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |████████████        |  151/245  batches, 3.33 cost, 7.71s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |████████████        |  153/245  batches, 3.33 cost, 7.81s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |████████████        |  155/245  batches, 3.31 cost, 7.91s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |████████████        |  158/245  batches, 3.28 cost, 8.06s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |█████████████       |  160/245  batches, 3.28 cost, 8.16s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |█████████████       |  163/245  batches, 3.26 cost, 8.31s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |█████████████       |  165/245  batches, 3.26 cost, 8.41s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |█████████████       |  168/245  batches, 3.25 cost, 8.56s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |█████████████       |  170/245  batches, 3.24 cost, 8.66s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |██████████████      |  173/245  batches, 3.23 cost, 8.81s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |██████████████      |  175/245  batches, 3.22 cost, 8.92s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |██████████████      |  178/245  batches, 3.18 cost, 9.07s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |██████████████      |  180/245  batches, 3.17 cost, 9.17s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |██████████████      |  182/245  batches, 3.17 cost, 9.27s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |███████████████     |  184/245  batches, 3.17 cost, 9.37s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |███████████████     |  187/245  batches, 3.16 cost, 9.52s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |███████████████     |  189/245  batches, 3.13 cost, 9.62s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |███████████████     |  192/245  batches, 3.10 cost, 9.77s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |███████████████     |  194/245  batches, 3.10 cost, 9.87s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |████████████████    |  197/245  batches, 3.09 cost, 10.02s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |████████████████    |  200/245  batches, 3.11 cost, 10.17s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |████████████████    |  202/245  batches, 3.08 cost, 10.27s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |████████████████    |  204/245  batches, 3.05 cost, 10.37s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |████████████████    |  207/245  batches, 3.02 cost, 10.52s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |█████████████████   |  209/245  batches, 3.00 cost, 10.62s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |█████████████████   |  211/245  batches, 3.00 cost, 10.72s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |█████████████████   |  214/245  batches, 2.99 cost, 10.87s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |█████████████████   |  216/245  batches, 3.00 cost, 10.97s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |█████████████████   |  219/245  batches, 3.02 cost, 11.12s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |██████████████████  |  221/245  batches, 3.02 cost, 11.22s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |██████████████████  |  224/245  batches, 2.99 cost, 11.37s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |██████████████████  |  226/245  batches, 2.94 cost, 11.47s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |██████████████████  |  229/245  batches, 2.90 cost, 11.62s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |██████████████████  |  231/245  batches, 2.88 cost, 11.72s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |███████████████████ |  234/245  batches, 2.88 cost, 11.87s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |███████████████████ |  236/245  batches, 2.91 cost, 11.97s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |███████████████████ |  239/245  batches, 2.91 cost, 12.12s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |███████████████████ |  241/245  batches, 2.92 cost, 12.22s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |███████████████████ |  244/245  batches, 2.90 cost, 12.37s]minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 1   [Train |████████████████████|  245/245  batches, 2.90 cost, 12.42s]minibatch_end\n",
      " [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "epoch_end\n",
      "epoch_begin\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |                    |    3/245  batches, 2.87 cost, 0.12s] minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |                    |    5/245  batches, 2.87 cost, 0.22s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2   [Train |                    |    8/245  batches, 2.83 cost, 0.37s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |                    |   10/245  batches, 2.80 cost, 0.47s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |█                   |   13/245  batches, 2.77 cost, 0.62s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |█                   |   15/245  batches, 2.77 cost, 0.72s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |█                   |   18/245  batches, 2.74 cost, 0.87s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |█                   |   20/245  batches, 2.77 cost, 0.97s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |█                   |   23/245  batches, 2.78 cost, 1.12s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |██                  |   25/245  batches, 2.75 cost, 1.22s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |██                  |   27/245  batches, 2.70 cost, 1.32s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |██                  |   30/245  batches, 2.72 cost, 1.47s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |██                  |   32/245  batches, 2.76 cost, 1.57s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |██                  |   35/245  batches, 2.67 cost, 1.72s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |███                 |   37/245  batches, 2.69 cost, 1.82s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |███                 |   39/245  batches, 2.63 cost, 1.92s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |███                 |   42/245  batches, 2.58 cost, 2.07s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |███                 |   44/245  batches, 2.62 cost, 2.17s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |███                 |   46/245  batches, 2.64 cost, 2.27s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |███                 |   48/245  batches, 2.65 cost, 2.37s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |████                |   50/245  batches, 2.66 cost, 2.47s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |████                |   52/245  batches, 2.63 cost, 2.57s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |████                |   54/245  batches, 2.59 cost, 2.67s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |████                |   56/245  batches, 2.58 cost, 2.77s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |████                |   59/245  batches, 2.56 cost, 2.92s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |████                |   61/245  batches, 2.54 cost, 3.03s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |█████               |   63/245  batches, 2.62 cost, 3.13s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |█████               |   66/245  batches, 2.62 cost, 3.28s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |█████               |   68/245  batches, 2.61 cost, 3.38s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |█████               |   70/245  batches, 2.59 cost, 3.48s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |█████               |   73/245  batches, 2.58 cost, 3.63s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |██████              |   75/245  batches, 2.56 cost, 3.73s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |██████              |   77/245  batches, 2.58 cost, 3.83s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |██████              |   80/245  batches, 2.58 cost, 3.98s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |██████              |   82/245  batches, 2.56 cost, 4.08s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |██████              |   85/245  batches, 2.55 cost, 4.23s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |███████             |   87/245  batches, 2.55 cost, 4.33s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |███████             |   90/245  batches, 2.54 cost, 4.48s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |███████             |   92/245  batches, 2.52 cost, 4.58s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |███████             |   95/245  batches, 2.53 cost, 4.73s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |███████             |   97/245  batches, 2.53 cost, 4.83s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |████████            |  100/245  batches, 2.54 cost, 4.98s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |████████            |  102/245  batches, 2.57 cost, 5.08s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |████████            |  105/245  batches, 2.54 cost, 5.23s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |████████            |  107/245  batches, 2.52 cost, 5.33s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |████████            |  109/245  batches, 2.50 cost, 5.43s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |█████████           |  112/245  batches, 2.43 cost, 5.58s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |█████████           |  114/245  batches, 2.43 cost, 5.68s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |█████████           |  116/245  batches, 2.43 cost, 5.78s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |█████████           |  119/245  batches, 2.41 cost, 5.93s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |█████████           |  122/245  batches, 2.41 cost, 6.08s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |██████████          |  124/245  batches, 2.40 cost, 6.18s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |██████████          |  126/245  batches, 2.41 cost, 6.28s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |██████████          |  129/245  batches, 2.44 cost, 6.43s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |██████████          |  132/245  batches, 2.40 cost, 6.58s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |██████████          |  134/245  batches, 2.40 cost, 6.68s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |███████████         |  136/245  batches, 2.36 cost, 6.78s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |███████████         |  139/245  batches, 2.30 cost, 6.93s]minibatch_end\n",
      "minibatch_begin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |███████████         |  141/245  batches, 2.32 cost, 7.03s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |███████████         |  144/245  batches, 2.30 cost, 7.18s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |███████████         |  146/245  batches, 2.31 cost, 7.28s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |████████████        |  149/245  batches, 2.30 cost, 7.43s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |████████████        |  151/245  batches, 2.31 cost, 7.53s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |████████████        |  154/245  batches, 2.33 cost, 7.68s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |████████████        |  156/245  batches, 2.32 cost, 7.78s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |████████████        |  159/245  batches, 2.34 cost, 7.93s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |█████████████       |  161/245  batches, 2.29 cost, 8.04s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |█████████████       |  163/245  batches, 2.29 cost, 8.14s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |█████████████       |  166/245  batches, 2.30 cost, 8.29s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |█████████████       |  168/245  batches, 2.26 cost, 8.39s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |█████████████       |  170/245  batches, 2.29 cost, 8.49s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |██████████████      |  173/245  batches, 2.28 cost, 8.64s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |██████████████      |  175/245  batches, 2.25 cost, 8.74s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |██████████████      |  178/245  batches, 2.21 cost, 8.89s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |██████████████      |  181/245  batches, 2.15 cost, 9.04s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |██████████████      |  183/245  batches, 2.17 cost, 9.14s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |███████████████     |  185/245  batches, 2.21 cost, 9.24s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |███████████████     |  187/245  batches, 2.22 cost, 9.34s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |███████████████     |  190/245  batches, 2.29 cost, 9.49s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |███████████████     |  192/245  batches, 2.29 cost, 9.59s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |███████████████     |  194/245  batches, 2.28 cost, 9.69s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |████████████████    |  196/245  batches, 2.32 cost, 9.79s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |████████████████    |  199/245  batches, 2.31 cost, 9.94s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |████████████████    |  201/245  batches, 2.33 cost, 10.04s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |████████████████    |  204/245  batches, 2.33 cost, 10.19s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |████████████████    |  206/245  batches, 2.30 cost, 10.29s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |█████████████████   |  209/245  batches, 2.23 cost, 10.44s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |█████████████████   |  211/245  batches, 2.22 cost, 10.54s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |█████████████████   |  214/245  batches, 2.22 cost, 10.70s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |█████████████████   |  217/245  batches, 2.25 cost, 10.84s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |█████████████████   |  219/245  batches, 2.25 cost, 10.95s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |██████████████████  |  222/245  batches, 2.26 cost, 11.10s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |██████████████████  |  224/245  batches, 2.21 cost, 11.20s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |██████████████████  |  226/245  batches, 2.19 cost, 11.30s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |██████████████████  |  229/245  batches, 2.17 cost, 11.45s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |██████████████████  |  232/245  batches, 2.11 cost, 11.60s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |███████████████████ |  234/245  batches, 2.15 cost, 11.70s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |███████████████████ |  236/245  batches, 2.18 cost, 11.80s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |███████████████████ |  239/245  batches, 2.19 cost, 11.95s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |███████████████████ |  242/245  batches, 2.21 cost, 12.10s]minibatch_end\n",
      "minibatch_begin\n",
      "minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |███████████████████ |  244/245  batches, 2.20 cost, 12.20s]minibatch_end\n",
      "minibatch_begin\n",
      "Epoch 2   [Train |████████████████████|  245/245  batches, 2.23 cost, 12.25s]minibatch_end\n",
      " [CrossEntropyMulti Loss 0.00, 0.00s]\n",
      "epoch_end\n",
      "train_end\n",
      "Neon training finishes in 41.12 seconds.\n",
      "Misclassification error = 54.2%. Finished in 0.78 seconds.\n",
      "Top 3 Misclassification error = 32.5%. Finished in 0.78 seconds.\n",
      "Misclassification error = 57.3% on test set. Finished in 1.24 seconds.\n",
      "Top 3 Misclassification error = 37.0% on test set. Finished in 1.25 seconds.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp = None\n",
    "epoch_num = 3\n",
    "neon_gaussInit = Gaussian(loc=0.0, scale=0.01)\n",
    "d = dict()\n",
    "b = \"gpu\"\n",
    "\n",
    "# Set up backend\n",
    "# backend: 'cpu' for single, 'mkl' for multi-thread cpu, and 'gpu' for gpu\n",
    "be = gen_backend(backend=b, batch_size=batch_size, rng_seed=542, datatype=np.float32)\n",
    "\n",
    "# Make iterators\n",
    "x_train, x_valid, neon_y_train, neon_y_valid = ms.train_test_split(trainImages, trainLabels, test_size=0.2, random_state=542)\n",
    "neon_train_set = ArrayIterator(X=np.asarray([t.flatten().astype('float32')/255 for t in x_train]), y=np.asarray(neon_y_train), make_onehot=True, nclass=43, lshape=(3, resize_size[0], resize_size[1]))\n",
    "neon_valid_set = ArrayIterator(X=np.asarray([t.flatten().astype('float32')/255 for t in x_valid]), y=np.asarray(neon_y_valid), make_onehot=True, nclass=43, lshape=(3, resize_size[0], resize_size[1]))\n",
    "neon_test_set = ArrayIterator(X=np.asarray([t.flatten().astype('float32')/255 for t in testImages]), y=np.asarray(testLabels), make_onehot=True, nclass=43, lshape=(3, resize_size[0], resize_size[1]))\n",
    "\n",
    "# Construct CNN\n",
    "layers = []\n",
    "layers.append(neon_Conv((5, 5, 64), strides=2, init=neon_gaussInit, bias=Constant(0.0), activation=Rectlin(), name=\"neon_conv1\"))\n",
    "layers.append(neon_Pooling(2, op=\"max\", strides=2, name=\"neon_pool1\"))\n",
    "layers.append(neon_Conv((3, 3, 512), strides=1, padding=1, init=neon_gaussInit, bias=Constant(0.0), activation=Rectlin(), name=\"neon_conv2\"))\n",
    "layers.append(neon_Pooling(2, op=\"max\", strides=2, name=\"neon_pool2\"))\n",
    "#     layers.append(neon_Pooling(5, op=\"avg\", name=\"neon_global_pool\"))\n",
    "layers.append(Affine(nout=4096, init=neon_gaussInit, bias=Constant(0.0), activation=Rectlin(), name=\"neon_fc1\"))\n",
    "layers.append(neon_Dropout(keep=0.5, name=\"neon_drop_out\"))\n",
    "layers.append(Affine(nout=43, init=neon_gaussInit, bias=Constant(0.0), activation=Softmax(), name=\"neon_fc2\"))\n",
    "\n",
    "# Initialize model object\n",
    "mlp = Model(layers=layers)\n",
    "\n",
    "# Costs\n",
    "neon_cost = GeneralizedCost(costfunc=CrossEntropyMulti())\n",
    "\n",
    "# Model summary\n",
    "mlp.initialize(neon_train_set, neon_cost)\n",
    "#     print(mlp)\n",
    "\n",
    "# Learning rules\n",
    "neon_optimizer = neon_SGD(0.01, momentum_coef=0.9, schedule=ExpSchedule(0.2))\n",
    "\n",
    "# Benchmark for 20 minibatches\n",
    "# d[b] = mlp.benchmark(neon_train_set, cost=neon_cost, optimizer=neon_optimizer)\n",
    "\n",
    "# Callbacks: validate on validation set\n",
    "callbacks = Callbacks(mlp, eval_set=neon_valid_set, metric=Misclassification(3), output_file=root+\"/callback_data_{}.h5\".format(b))\n",
    "callbacks.add_callback(LossCallback(eval_set=neon_test_set, epoch_freq=1))\n",
    "callbacks.add_callback(SelfCallback(epoch_freq=1, minibatch_freq=1))\n",
    "\n",
    "# Fit\n",
    "start = time.time()\n",
    "mlp.fit(neon_train_set, optimizer=neon_optimizer, num_epochs=epoch_num, cost=neon_cost, callbacks=callbacks)\n",
    "print(\"Neon training finishes in {:.2f} seconds.\".format(time.time() - start))\n",
    "\n",
    "# Result\n",
    "results = mlp.get_outputs(neon_valid_set)\n",
    "\n",
    "# Print error on validation set\n",
    "start = time.time()\n",
    "neon_error_mis = mlp.eval(neon_valid_set, metric=Misclassification())*100\n",
    "print('Misclassification error = {:.1f}%. Finished in {:.2f} seconds.'.format(neon_error_mis[0], time.time() - start))\n",
    "\n",
    "start = time.time()\n",
    "neon_error_top3 = mlp.eval(neon_valid_set, metric=TopKMisclassification(3))*100\n",
    "print('Top 3 Misclassification error = {:.1f}%. Finished in {:.2f} seconds.'.format(neon_error_top3[2], time.time() - start))\n",
    "\n",
    "mlp.save_params(root + \"/saved_models/neon_weights_{}.prm\".format(b))\n",
    "\n",
    "# Print error on test set\n",
    "start = time.time()\n",
    "neon_error_mis_t = mlp.eval(neon_test_set, metric=Misclassification())*100\n",
    "print('Misclassification error = {:.1f}% on test set. Finished in {:.2f} seconds.'.format(neon_error_mis_t[0], time.time() - start))\n",
    "\n",
    "start = time.time()\n",
    "neon_error_top3_t = mlp.eval(neon_test_set, metric=TopKMisclassification(3))*100\n",
    "print('Top 3 Misclassification error = {:.1f}% on test set. Finished in {:.2f} seconds.'.format(neon_error_top3_t[2], time.time() - start))\n",
    "\n",
    "cleanup_backend()\n",
    "mlp = None\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp = None\n",
    "epoch_num = 3\n",
    "\n",
    "# if os.path.isfile(root + \"/saved_models/neon_weights.prm\"):\n",
    "#     print(\"Model exists\")\n",
    "# else:\n",
    "\n",
    "neon_backends = [\"cpu\", \"mkl\", \"gpu\"]\n",
    "neon_gaussInit = Gaussian(loc=0.0, scale=0.01)\n",
    "\n",
    "d = dict()\n",
    "\n",
    "for b in neon_backends:\n",
    "    print(\"Use {} as backend.\".format(b))\n",
    "    \n",
    "    # Set up backend\n",
    "    # backend: 'cpu' for single, 'mkl' for multi-thread cpu, and 'gpu' for gpu\n",
    "    be = gen_backend(backend=b, batch_size=batch_size, rng_seed=542, datatype=np.float32)\n",
    "\n",
    "    # Make iterators\n",
    "    x_train, x_valid, neon_y_train, neon_y_valid = ms.train_test_split(trainImages, trainLabels, test_size=0.2, random_state=542)\n",
    "    neon_train_set = ArrayIterator(X=np.asarray([t.flatten().astype('float32')/255 for t in x_train]), y=np.asarray(neon_y_train), make_onehot=True, nclass=43, lshape=(3, resize_size[0], resize_size[1]))\n",
    "    neon_valid_set = ArrayIterator(X=np.asarray([t.flatten().astype('float32')/255 for t in x_valid]), y=np.asarray(neon_y_valid), make_onehot=True, nclass=43, lshape=(3, resize_size[0], resize_size[1]))\n",
    "    neon_test_set = ArrayIterator(X=np.asarray([t.flatten().astype('float32')/255 for t in testImages]), y=np.asarray(testLabels), make_onehot=True, nclass=43, lshape=(3, resize_size[0], resize_size[1]))\n",
    "    \n",
    "    # Construct CNN\n",
    "    layers = []\n",
    "    layers.append(neon_Conv((5, 5, 64), strides=2, init=neon_gaussInit, bias=Constant(0.0), activation=Rectlin(), name=\"neon_conv1\"))\n",
    "    layers.append(neon_Pooling(2, op=\"max\", strides=2, name=\"neon_pool1\"))\n",
    "    layers.append(neon_Conv((3, 3, 512), strides=1, padding=1, init=neon_gaussInit, bias=Constant(0.0), activation=Rectlin(), name=\"neon_conv2\"))\n",
    "    layers.append(neon_Pooling(2, op=\"max\", strides=2, name=\"neon_pool2\"))\n",
    "#     layers.append(neon_Pooling(5, op=\"avg\", name=\"neon_global_pool\"))\n",
    "    layers.append(Affine(nout=4096, init=neon_gaussInit, bias=Constant(0.0), activation=Rectlin(), name=\"neon_fc1\"))\n",
    "    layers.append(neon_Dropout(keep=0.5, name=\"neon_drop_out\"))\n",
    "    layers.append(Affine(nout=43, init=neon_gaussInit, bias=Constant(0.0), activation=Softmax(), name=\"neon_fc2\"))\n",
    "\n",
    "    # Initialize model object\n",
    "    mlp = Model(layers=layers)\n",
    "\n",
    "    # Costs\n",
    "    neon_cost = GeneralizedCost(costfunc=CrossEntropyMulti())\n",
    "\n",
    "    # Model summary\n",
    "    mlp.initialize(neon_train_set, neon_cost)\n",
    "#     print(mlp)\n",
    "\n",
    "    # Learning rules\n",
    "    neon_optimizer = neon_SGD(0.01, momentum_coef=0.9, schedule=ExpSchedule(0.2))\n",
    "\n",
    "    # Benchmark for 20 minibatches\n",
    "    d[b] = mlp.benchmark(neon_train_set, cost=neon_cost, optimizer=neon_optimizer)\n",
    "    \n",
    "    # Callbacks: validate on validation set\n",
    "    callbacks = Callbacks(mlp, eval_set=neon_valid_set, metric=Misclassification(3), output_file=root+\"/callback_data_{}.h5\".format(b))\n",
    "    callbacks.add_callback(LossCallback(eval_set=neon_test_set, epoch_freq=1))\n",
    "\n",
    "    # Fit\n",
    "    start = time.time()\n",
    "    mlp.fit(neon_train_set, optimizer=neon_optimizer, num_epochs=epoch_num, cost=neon_cost, callbacks=callbacks)\n",
    "    print(\"Neon training finishes in {:.2f} seconds.\".format(time.time() - start))\n",
    "\n",
    "    # Result\n",
    "    results = mlp.get_outputs(neon_valid_set)\n",
    "\n",
    "    # Print error on validation set\n",
    "    start = time.time()\n",
    "    neon_error_mis = mlp.eval(neon_valid_set, metric=Misclassification())*100\n",
    "    print('Misclassification error = {:.1f}%. Finished in {:.2f} seconds.'.format(neon_error_mis[0], time.time() - start))\n",
    "\n",
    "    start = time.time()\n",
    "    neon_error_top3 = mlp.eval(neon_valid_set, metric=TopKMisclassification(3))*100\n",
    "    print('Top 3 Misclassification error = {:.1f}%. Finished in {:.2f} seconds.'.format(neon_error_top3[2], time.time() - start))\n",
    "\n",
    "    mlp.save_params(root + \"/saved_models/neon_weights_{}.prm\".format(b))\n",
    "\n",
    "    # Print error on test set\n",
    "    start = time.time()\n",
    "    neon_error_mis_t = mlp.eval(neon_test_set, metric=Misclassification())*100\n",
    "    print('Misclassification error = {:.1f}% on test set. Finished in {:.2f} seconds.'.format(neon_error_mis_t[0], time.time() - start))\n",
    "\n",
    "    start = time.time()\n",
    "    neon_error_top3_t = mlp.eval(neon_test_set, metric=TopKMisclassification(3))*100\n",
    "    print('Top 3 Misclassification error = {:.1f}% on test set. Finished in {:.2f} seconds.'.format(neon_error_top3_t[2], time.time() - start))\n",
    "\n",
    "    cleanup_backend()\n",
    "    mlp = None\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Draw figures\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from neon.visualizations.data import h5_cost_data\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (10,8)\n",
    "\n",
    "train_cost = pd.DataFrame()\n",
    "for b in neon_backends:\n",
    "    f = h5py.File(root+\"/callback_data_{}.h5\".format(b), \"r\")\n",
    "    keys = list(f['.'].keys())\n",
    "    \n",
    "    train_cost['neon_{}'.format(b)] = pd.Series(f['.']['cost']['train'][()])\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "plt.figure()\n",
    "train_cost.plot()\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "plt.savefig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras with different multiple backends (Tensorflow, Theano, CNTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D as keras_Conv, MaxPooling2D as keras_MaxPooling, GlobalAveragePooling2D as keras_AveragePooling\n",
    "from keras.layers import Dropout as keras_Dropout, Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import image\n",
    "from keras.initializers import RandomNormal, Constant as keras_Constant\n",
    "from keras.optimizers import SGD as keras_SGD\n",
    "from keras.callbacks import ModelCheckpoint, Callback as keras_callback\n",
    "from sklearn import model_selection as ms\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.layers.convolutional import ZeroPadding2D\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LossHistory(keras_callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to dynamically change keras backend\n",
    "from importlib import reload\n",
    "def set_keras_backend(backend):\n",
    "    if K.backend() != backend:\n",
    "        os.environ['KERAS_BACKEND'] = backend\n",
    "        reload(K)\n",
    "        assert K.backend() == backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sys import platform\n",
    "backends = [\"theano\", \"tensorflow\"]\n",
    "if platform != \"darwin\":\n",
    "    backends.append(\"cntk\")\n",
    "    \n",
    "for b in backends:\n",
    "    set_keras_backend(b)\n",
    "\n",
    "    # Load and process images\n",
    "    enc = OneHotEncoder(sparse=False)\n",
    "    x_train, x_valid, y_train, y_valid = ms.train_test_split(trainImages, trainLabels, test_size=0.2, random_state=542)\n",
    "    keras_train_x = np.vstack([np.expand_dims(image.img_to_array(x), axis=0).astype('float32')/255 for x in x_train])\n",
    "    keras_valid_x = np.vstack([np.expand_dims(image.img_to_array(x), axis=0).astype('float32')/255 for x in x_valid])\n",
    "    keras_test_x = np.vstack([np.expand_dims(image.img_to_array(x), axis=0).astype('float32')/255 for x in testImages])\n",
    "    keras_train_y = to_categorical(y_train, 43)\n",
    "    keras_valid_y = to_categorical(y_valid, 43)\n",
    "    keras_test_y = to_categorical(testLabels, 43)\n",
    "\n",
    "    # Build model\n",
    "    keras_gaussInit = RandomNormal(mean=0.0, stddev=0.01, seed=542)\n",
    "    layer_name_prefix = b+\"_\"\n",
    "\n",
    "    keras_model = Sequential()\n",
    "    keras_model.add(keras_Conv(64, (5, 5), kernel_initializer=keras_gaussInit, strides=(2, 2), bias_initializer=keras_Constant(0.0), activation=\"relu\", input_shape=(resize_size[0], resize_size[1], 3), name=layer_name_prefix+\"conv1\"))\n",
    "    keras_model.add(keras_MaxPooling(pool_size=(2, 2), name=layer_name_prefix+\"pool1\"))\n",
    "    # keras_model.add(ZeroPadding2D(padding=(1, 1), name=layer_name_prefix+\"zero_padding\"))\n",
    "    # keras_model.add(keras_Conv(256, (3, 3), kernel_initializer=keras_gaussInit, strides=(1, 1), bias_initializer=keras_Constant(0.0), activation=\"relu\", name=layer_name_prefix+\"conv2\"))\n",
    "    keras_model.add(keras_Conv(256, (3, 3), kernel_initializer=keras_gaussInit, strides=(1, 1), padding=\"same\", bias_initializer=keras_Constant(0.0), activation=\"relu\", name=layer_name_prefix+\"conv2\"))\n",
    "    keras_model.add(keras_MaxPooling(pool_size=(2, 2), name=layer_name_prefix+\"pool2\"))\n",
    "    # keras_model.add(keras_AveragePooling(name=layer_name_prefix+\"global_pool\"))\n",
    "    keras_model.add(Flatten(name=layer_name_prefix+\"flatten\")) # An extra layer to flatten the previous layer in order to connect to fully connected layer\n",
    "    keras_model.add(Dense(4096, kernel_initializer=keras_gaussInit, bias_initializer=keras_Constant(0.0), activation=\"relu\", name=layer_name_prefix+\"fc1\"))\n",
    "    keras_model.add(keras_Dropout(0.5, name=layer_name_prefix+\"drop_out\"))\n",
    "    keras_model.add(Dense(43, kernel_initializer=keras_gaussInit, bias_initializer=keras_Constant(0.0), activation=\"softmax\", name=layer_name_prefix+\"fc2\"))\n",
    "    keras_model.summary()\n",
    "\n",
    "    keras_optimizer = keras_SGD(lr=0.01, decay=1.6e-8, momentum=0.9)\n",
    "    keras_cost = \"categorical_crossentropy\"\n",
    "    keras_model.compile(loss=keras_cost, optimizer=keras_optimizer, metrics=[\"acc\"])\n",
    "\n",
    "    checkpointer = ModelCheckpoint(filepath=root+\"/saved_models/keras_\"+b+\"_weights.hdf5\",\n",
    "                                       verbose=1, save_best_only=True)\n",
    "    losses = LossHistory()\n",
    "\n",
    "    start = time.time()\n",
    "    keras_model.fit(keras_train_x, keras_train_y,\n",
    "                  validation_data=(keras_valid_x, keras_valid_y),\n",
    "                  epochs=epoch_num, batch_size=batch_size, callbacks=[checkpointer, losses], verbose=1, shuffle=True)\n",
    "    print(\"{} training finishes in {:.2f} seconds.\".format(b, time.time() - start))\n",
    "    \n",
    "    train_cost['keras_{}'.format(b)] = pd.Series(\"f['.']['cost']['train'][()]\")\n",
    "\n",
    "    keras_model.load_weights(root+\"/saved_models/keras_\"+b+\"_weights.hdf5\")\n",
    "    keras_predictions = [np.argmax(keras_model.predict(np.expand_dims(feature, axis=0))) for feature in keras_test_x]\n",
    "\n",
    "    # report test accuracy\n",
    "    keras_test_accuracy = 100*np.sum(np.array(keras_predictions)==np.argmax(keras_test_y, axis=1))/len(keras_predictions)\n",
    "    print('{} test accuracy: {:.1f}%'.format(b, keras_test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as utils\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        # Build model\n",
    "        self.conv = torch.nn.Sequential()\n",
    "        self.conv.add_module(\"torch_conv1\", torch.nn.Conv2d(3, 64, kernel_size=(5, 5), stride=2))\n",
    "        self.conv.add_module(\"torch_pool1\", torch.nn.MaxPool2d(kernel_size=2))\n",
    "        self.conv.add_module(\"torch_relu1\", torch.nn.ReLU())\n",
    "        self.conv.add_module(\"torch_conv2\", torch.nn.Conv2d(64, 256, kernel_size=(3, 3), stride=1, padding=1))\n",
    "        self.conv.add_module(\"torch_pool2\", torch.nn.MaxPool2d(kernel_size=2))\n",
    "        self.conv.add_module(\"torch_relu2\", torch.nn.ReLU())\n",
    "        self.conv.add_module(\"torch_global_pool\", torch.nn.AvgPool2d(kernel_size=5))\n",
    "        \n",
    "        self.csf = torch.nn.Sequential()\n",
    "        self.csf.add_module(\"torch_fc1\", torch.nn.Linear(256, 4096))\n",
    "        self.csf.add_module(\"torch_relu3\", torch.nn.ReLU())\n",
    "        self.csf.add_module(\"torch_dropout1\", torch.nn.Dropout(0.5))\n",
    "        self.csf.add_module(\"torch_fc2\", torch.nn.Linear(4096, 43))\n",
    "        \n",
    "        # Initialize conv layers and fc layers\n",
    "        torch.nn.init.normal(self.conv.state_dict()[\"torch_conv1.weight\"], mean=0, std=0.01)\n",
    "        torch.nn.init.constant(self.conv.state_dict()[\"torch_conv1.bias\"], 0.0)\n",
    "        torch.nn.init.normal(self.conv.state_dict()[\"torch_conv2.weight\"], mean=0, std=0.01)\n",
    "        torch.nn.init.constant(self.conv.state_dict()[\"torch_conv2.bias\"], 0.0)\n",
    "        torch.nn.init.normal(self.csf.state_dict()[\"torch_fc1.weight\"], mean=0, std=0.01)\n",
    "        torch.nn.init.constant(self.csf.state_dict()[\"torch_fc1.bias\"], 0.0)\n",
    "        torch.nn.init.normal(self.csf.state_dict()[\"torch_fc2.weight\"], mean=0, std=0.01)\n",
    "        torch.nn.init.constant(self.csf.state_dict()[\"torch_fc2.bias\"], 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv.forward(x)\n",
    "        x = x.view(-1, 256)\n",
    "        return self.csf.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = ms.train_test_split(trainImages, trainLabels, test_size=0.2, random_state=542)\n",
    "\n",
    "torch_train_x = torch.stack([torch.Tensor(i.swapaxes(0,2).astype(\"float32\")/255) for i in x_train])\n",
    "torch_train_y = torch.LongTensor(y_train)\n",
    "torch_valid_x = torch.stack([torch.Tensor(i.swapaxes(0,2).astype(\"float32\")/255) for i in x_valid])\n",
    "torch_valid_y = torch.LongTensor(y_valid)\n",
    "torch_test_x = torch.stack([torch.Tensor(i.swapaxes(0,2).astype(\"float32\")/255) for i in testImages])\n",
    "torch_test_y = torch.LongTensor(testLabels)\n",
    "\n",
    "torch_tensor_train_set = utils.TensorDataset(torch_train_x, torch_train_y)\n",
    "torch_train_set = utils.DataLoader(torch_tensor_train_set, batch_size=batch_size, shuffle=True)\n",
    "torch_tensor_valid_set = utils.TensorDataset(torch_valid_x, torch_valid_y)\n",
    "torch_valid_set = utils.DataLoader(torch_tensor_valid_set, batch_size=batch_size, shuffle=True)\n",
    "torch_tensor_test_set = utils.TensorDataset(torch_test_x, torch_test_y)\n",
    "torch_test_set = utils.DataLoader(torch_tensor_test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "torch_model = ConvNet()\n",
    "optimizer = optim.SGD(torch_model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    torch_model.train()\n",
    "    for batch_idx, (data, target) in enumerate(torch_train_set):\n",
    "#         if args.cuda:\n",
    "#             data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = torch_model(data)\n",
    "        cost = torch.nn.CrossEntropyLoss(size_average=True)\n",
    "        loss = cost(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(torch_train_set.dataset),\n",
    "                100. * batch_idx / len(torch_train_set), loss.data[0]))\n",
    "def test():\n",
    "    torch_model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in torch_test_set:\n",
    "#         if args.cuda:\n",
    "#             data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = torch_model(data)\n",
    "        cost = torch.nn.CrossEntropyLoss(size_average=False)\n",
    "        test_loss += cost(output, target).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(torch_test_set.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(torch_test_set.dataset),\n",
    "        100. * correct / len(torch_test_set.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, epoch_num + 1):\n",
    "    train(epoch)\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
