{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Comparison of Deep Learning Frameworks\n",
    "\n",
    "## Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn import model_selection as ms\n",
    "from sys import platform\n",
    "import DLHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/moderato/Downloads/GTSRB/try\n",
      "Training Image preprocessing finished in 18.09 seconds\n",
      "Testing Image preprocessing finished in 5.88 seconds\n",
      "(48, 48, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\no4LYwjeMCmIL3zAqyP8HJS0m78LUXioAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1047afe80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 48, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/H/c02PgpXPu0gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113b65be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if platform == \"darwin\":\n",
    "    root = \"/Users/moderato/Downloads/GTSRB/try\"\n",
    "else:\n",
    "    root = \"/home/zhongyilin/Desktop/GTSRB/try\"\n",
    "print(root)\n",
    "resize_size = (48, 48)\n",
    "epoch_num = 25\n",
    "batch_size = 64\n",
    "trainImages, trainLabels, testImages, testLabels = DLHelper.getImageSets(root, resize_size, 'clahe')\n",
    "x_train, x_valid, y_train, y_valid = ms.train_test_split(trainImages, trainLabels, test_size=0.2, random_state=542)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a,b,c,d = DLHelper.getImageSets(root, resize_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intel Nervana Neon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neon.backends import gen_backend, cleanup_backend\n",
    "from neon.initializers import Gaussian, Constant, GlorotUniform\n",
    "from neon.layers import GeneralizedCost, Affine\n",
    "from neon.layers import Conv as neon_Conv, Dropout as neon_Dropout, Pooling as neon_Pooling\n",
    "from neon.transforms import Rectlin, Softmax, CrossEntropyMulti, Misclassification, TopKMisclassification\n",
    "from neon.models import Model\n",
    "from neon.optimizers import GradientDescentMomentum as neon_SGD, RMSProp as neon_RMSProp, ExpSchedule\n",
    "from neon.callbacks.callbacks import Callbacks, Callback, LossCallback\n",
    "from neon.data.dataiterator import ArrayIterator\n",
    "from timeit import default_timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SelfCallback(LossCallback):\n",
    "    def __init__(self, eval_set, epoch_freq):\n",
    "        super(SelfCallback, self).__init__(eval_set=eval_set, epoch_freq=epoch_freq)\n",
    "        self.train_batch_time = None\n",
    "        self.total_batch_index = 0\n",
    "        \n",
    "    def on_train_begin(self, callback_data, model, epochs):\n",
    "        super(SelfCallback, self).on_train_begin(callback_data, model, epochs)\n",
    "        \n",
    "        # Save training time per batch\n",
    "        points = callback_data['config'].attrs['total_minibatches']\n",
    "        tb = callback_data.create_dataset(\"time/train_batch\", (points,))\n",
    "        tb.attrs['time_markers'] = 'minibatch'\n",
    "        \n",
    "    def on_minibatch_begin(self, callback_data, model, epoch, minibatch):\n",
    "        self.train_batch_time = default_timer()\n",
    "\n",
    "    def on_minibatch_end(self, callback_data, model, epoch, minibatch):\n",
    "        callback_data[\"time/train_batch\"][self.total_batch_index] = (default_timer() - self.train_batch_time)\n",
    "        self.total_batch_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp = None\n",
    "epoch_num = 50\n",
    "\n",
    "neon_backends = [\"cpu\", \"mkl\", \"gpu\"]\n",
    "neon_gaussInit = Gaussian(loc=0.0, scale=0.01)\n",
    "d = dict()\n",
    "neon_lr = {\"cpu\": 0.01, \"mkl\": 0.0005, \"gpu\": 0.01}\n",
    "run_or_not = {\"cpu\": False, \"mkl\": True, \"gpu\": False}\n",
    "\n",
    "cleanup_backend()\n",
    "\n",
    "for b in neon_backends:\n",
    "    if run_or_not[b]:\n",
    "        print(\"Use {} as backend.\".format(b))\n",
    "\n",
    "        # Set up backend\n",
    "        # backend: 'cpu' for single, 'mkl' for multi-thread cpu, and 'gpu' for gpu\n",
    "        be = gen_backend(backend=b, batch_size=batch_size, rng_seed=542, datatype=np.float32)\n",
    "        print(type(be))\n",
    "\n",
    "        # Make iterators\n",
    "        x_train, x_valid, neon_y_train, neon_y_valid = ms.train_test_split(trainImages, trainLabels, test_size=0.2, random_state=542)\n",
    "        neon_train_set = ArrayIterator(X=np.asarray([t.flatten().astype('float32')/255 for t in x_train]), y=np.asarray(neon_y_train), make_onehot=True, nclass=43, lshape=(3, resize_size[0], resize_size[1]))\n",
    "        neon_valid_set = ArrayIterator(X=np.asarray([t.flatten().astype('float32')/255 for t in x_valid]), y=np.asarray(neon_y_valid), make_onehot=True, nclass=43, lshape=(3, resize_size[0], resize_size[1]))\n",
    "        neon_test_set = ArrayIterator(X=np.asarray([t.flatten().astype('float32')/255 for t in testImages]), y=np.asarray(testLabels), make_onehot=True, nclass=43, lshape=(3, resize_size[0], resize_size[1]))\n",
    "\n",
    "        # Construct CNN\n",
    "        layers = []\n",
    "        layers.append(neon_Conv((5, 5, 64), strides=2, init=neon_gaussInit, bias=Constant(0.0), activation=Rectlin(), name=\"neon_conv1\"))\n",
    "        layers.append(neon_Pooling(2, op=\"max\", strides=2, name=\"neon_pool1\"))\n",
    "        layers.append(neon_Conv((3, 3, 512), strides=1, padding=1, init=neon_gaussInit, bias=Constant(0.0), activation=Rectlin(), name=\"neon_conv2\"))\n",
    "        layers.append(neon_Pooling(2, op=\"max\", strides=2, name=\"neon_pool2\"))\n",
    "    #     layers.append(neon_Pooling(5, op=\"avg\", name=\"neon_global_pool\"))\n",
    "        layers.append(Affine(nout=2048, init=neon_gaussInit, bias=Constant(0.0), activation=Rectlin(), name=\"neon_fc1\"))\n",
    "        layers.append(neon_Dropout(keep=0.5, name=\"neon_drop_out\"))\n",
    "        layers.append(Affine(nout=43, init=neon_gaussInit, bias=Constant(0.0), activation=Softmax(), name=\"neon_fc2\"))\n",
    "\n",
    "        # Initialize model object\n",
    "        mlp = Model(layers=layers)\n",
    "\n",
    "        # Costs\n",
    "        neon_cost = GeneralizedCost(costfunc=CrossEntropyMulti())\n",
    "\n",
    "        # Model summary\n",
    "        mlp.initialize(neon_train_set, neon_cost)\n",
    "        #     print(mlp)\n",
    "\n",
    "        # Learning rules\n",
    "\n",
    "        neon_optimizer = neon_SGD(neon_lr[b], momentum_coef=0.9, schedule=ExpSchedule(0.2))\n",
    "    #     neon_optimizer = neon_RMSProp(learning_rate=0.0001, decay_rate=0.95)\n",
    "\n",
    "        # Benchmark for 20 minibatches\n",
    "        d[b] = mlp.benchmark(neon_train_set, cost=neon_cost, optimizer=neon_optimizer)\n",
    "\n",
    "        # Reset model\n",
    "        mlp = None\n",
    "        mlp = Model(layers=layers)\n",
    "        mlp.initialize(neon_train_set, neon_cost)\n",
    "\n",
    "        # Callbacks: validate on validation set\n",
    "        callbacks = Callbacks(mlp, eval_set=neon_valid_set, metric=Misclassification(3), output_file=root+\"/callback_data_{}.h5\".format(b))\n",
    "        callbacks.add_callback(SelfCallback(eval_set=neon_valid_set, epoch_freq=1))\n",
    "\n",
    "        # Fit\n",
    "        start = time.time()\n",
    "        mlp.fit(neon_train_set, optimizer=neon_optimizer, num_epochs=epoch_num, cost=neon_cost, callbacks=callbacks)\n",
    "        print(\"Neon training finishes in {:.2f} seconds.\".format(time.time() - start))\n",
    "\n",
    "        # Result\n",
    "        results = mlp.get_outputs(neon_valid_set)\n",
    "\n",
    "        # Print error on validation set\n",
    "        start = time.time()\n",
    "        neon_error_mis = mlp.eval(neon_valid_set, metric=Misclassification())*100\n",
    "        print('Misclassification error = {:.1f}%. Finished in {:.2f} seconds.'.format(neon_error_mis[0], time.time() - start))\n",
    "\n",
    "        start = time.time()\n",
    "        neon_error_top3 = mlp.eval(neon_valid_set, metric=TopKMisclassification(3))*100\n",
    "        print('Top 3 Misclassification error = {:.1f}%. Finished in {:.2f} seconds.'.format(neon_error_top3[2], time.time() - start))\n",
    "\n",
    "        start = time.time()\n",
    "        neon_error_top5 = mlp.eval(neon_valid_set, metric=TopKMisclassification(5))*100\n",
    "        print('Top 5 Misclassification error = {:.1f}%. Finished in {:.2f} seconds.'.format(neon_error_top5[2], time.time() - start))\n",
    "\n",
    "        mlp.save_params(root + \"/saved_models/neon_weights_{}.prm\".format(b))\n",
    "\n",
    "        # Print error on test set\n",
    "        start = time.time()\n",
    "        neon_error_mis_t = mlp.eval(neon_test_set, metric=Misclassification())*100\n",
    "        print('Misclassification error = {:.1f}% on test set. Finished in {:.2f} seconds.'.format(neon_error_mis_t[0], time.time() - start))\n",
    "\n",
    "        start = time.time()\n",
    "        neon_error_top3_t = mlp.eval(neon_test_set, metric=TopKMisclassification(3))*100\n",
    "        print('Top 3 Misclassification error = {:.1f}% on test set. Finished in {:.2f} seconds.'.format(neon_error_top3_t[2], time.time() - start))\n",
    "\n",
    "        start = time.time()\n",
    "        neon_error_top5_t = mlp.eval(neon_test_set, metric=TopKMisclassification(5))*100\n",
    "        print('Top 5 Misclassification error = {:.1f}% on test set. Finished in {:.2f} seconds.'.format(neon_error_top5_t[2], time.time() - start))\n",
    "\n",
    "        cleanup_backend()\n",
    "        mlp = None\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Draw figures\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from neon.visualizations.data import h5_cost_data\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (10,8)\n",
    "\n",
    "train_cost_batch = pd.DataFrame()\n",
    "valid_cost_epoch = pd.DataFrame()\n",
    "train_epoch_mark = dict()\n",
    "# neon_backends = [\"mkl\", \"cpu\", \"gpu\"]\n",
    "\n",
    "fig1 = plt.figure()\n",
    "ax1 = fig1.add_subplot(1,1,1)\n",
    "fig2, ax2 = plt.subplots(1, len(neon_backends))\n",
    "markers = ['o', 'x', 'p']\n",
    "\n",
    "for i in range(len(neon_backends)):\n",
    "    b = neon_backends[i]\n",
    "    \n",
    "    f = h5py.File(root+\"/callback_data_{}.h5\".format(b), \"r\")\n",
    "    keys = list(f['.'].keys())\n",
    "    \n",
    "    train_cost_batch['neon_{}_loss'.format(b)] = pd.Series(f['.']['cost']['train'][()])\n",
    "    train_cost_batch['neon_{}_t'.format(b)] = pd.Series(f['.']['time']['train_batch'][()]).cumsum()\n",
    "    \n",
    "    valid_cost_epoch['neon_{}_loss'.format(b)] = pd.Series(f['.']['cost']['loss'][()])\n",
    "    valid_cost_epoch['neon_{}_t'.format(b)] = pd.Series(f['.']['time']['loss'][()])\n",
    "    \n",
    "    tmp = (f['.']['time_markers']['minibatch'][()]-1).astype(int).tolist()\n",
    "    tmp.pop()\n",
    "    tmp = [0] + tmp\n",
    "    train_epoch_mark['neon_{}_mark'.format(b)] = tmp\n",
    "    \n",
    "    ax1.plot(train_cost_batch['neon_{}_t'.format(b)].iloc[train_epoch_mark['neon_{}_mark'.format(b)]], \\\n",
    "             train_cost_batch['neon_{}_loss'.format(b)].iloc[train_epoch_mark['neon_{}_mark'.format(b)]], marker=markers[i])\n",
    "    \n",
    "    ax2[i].plot(range(len(train_epoch_mark['neon_{}_mark'.format(b)])), \\\n",
    "             train_cost_batch['neon_{}_loss'.format(b)].iloc[train_epoch_mark['neon_{}_mark'.format(b)]], marker=markers[0])\n",
    "    ax2[i].plot(range(len(train_epoch_mark['neon_{}_mark'.format(b)])), valid_cost_epoch['neon_{}_loss'.format(b)], marker=markers[1])\n",
    "    ax2[i].legend(loc='best')\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "ax1.legend(loc='best')\n",
    "plt.show()\n",
    "fig1.savefig(root+\"/pics/neon_train_loss_time.png\", dpi=fig1.dpi)\n",
    "fig2.savefig(root+\"/pics/neon_train_loss_epoch.png\", dpi=fig2.dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras with different multiple backends (Tensorflow, Theano, CNTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D as keras_Conv\n",
    "from keras.layers import MaxPooling2D as keras_MaxPooling, GlobalAveragePooling2D as keras_AveragePooling\n",
    "from keras.layers import Dropout as keras_Dropout, Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import image\n",
    "from keras.initializers import RandomNormal, Constant as keras_Constant\n",
    "from keras.optimizers import SGD as keras_SGD, RMSprop as keras_RMSProp\n",
    "from keras.callbacks import ModelCheckpoint, Callback as keras_callback\n",
    "from sklearn import model_selection as ms\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.layers.convolutional import ZeroPadding2D\n",
    "import os, h5py\n",
    "from timeit import default_timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LossHistory(keras_callback):\n",
    "    def __init__(self, filename, epoch_num, max_total_batch):\n",
    "        super(keras_callback, self).__init__()\n",
    "        \n",
    "        self.batch_count = 0\n",
    "        self.epoch_num = epoch_num\n",
    "        self.filename = filename\n",
    "        self.batch_time = None\n",
    "        self.max_total_batch = max_total_batch\n",
    "        \n",
    "        self.f = h5py.File(filename, 'w')\n",
    "        \n",
    "        try:\n",
    "            config = self.f.create_group('config')\n",
    "            config.attrs[\"total_epochs\"] = self.epoch_num\n",
    "\n",
    "            cost = self.f.create_group('cost')\n",
    "            loss = cost.create_dataset('loss', (self.epoch_num,))\n",
    "            loss.attrs['time_markers'] = 'epoch_freq'\n",
    "            loss.attrs['epoch_freq'] = 1\n",
    "            train = cost.create_dataset('train', (self.max_total_batch,)) # Set size to maximum theoretical value\n",
    "            train.attrs['time_markers'] = 'minibatch'\n",
    "\n",
    "            t = self.f.create_group('time')\n",
    "            loss = t.create_dataset('loss', (self.epoch_num,))\n",
    "            train = t.create_group('train')\n",
    "            start_time = train.create_dataset(\"start_time\", (1,))\n",
    "            start_time.attrs['units'] = 'seconds'\n",
    "            end_time = train.create_dataset(\"end_time\", (1,))\n",
    "            end_time.attrs['units'] = 'seconds'\n",
    "            train_batch = t.create_dataset('train_batch', (self.max_total_batch,)) # Same as above\n",
    "\n",
    "            time_markers = self.f.create_group('time_markers')\n",
    "            time_markers.attrs['epochs_complete'] = self.epoch_num\n",
    "            train_batch = time_markers.create_dataset('minibatch', (self.epoch_num,))\n",
    "        except Exception as e:\n",
    "            self.f.close() # Avoid hdf5 runtime error or os error\n",
    "            raise e # Catch the exception to close the file, then raise it to stop the program\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        try:\n",
    "            self.f['.']['time']['train']['start_time'][0] = default_timer()\n",
    "        except Exception as e:\n",
    "            self.f.close()\n",
    "            raise e\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        try:\n",
    "            self.f['.']['cost']['loss'][epoch] = np.float32(logs.get('val_loss'))\n",
    "            self.f['.']['time_markers']['minibatch'][epoch] = np.float32(self.batch_count)\n",
    "        except Exception as e:\n",
    "            self.f.close()\n",
    "            raise e\n",
    "        \n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        try:\n",
    "            self.batch_time = default_timer()\n",
    "        except Exception as e:\n",
    "            self.f.close()\n",
    "            raise e\n",
    "    \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        try:\n",
    "            self.f['.']['cost']['train'][self.batch_count] = np.float32(logs.get('loss'))\n",
    "            self.f['.']['time']['train_batch'][self.batch_count] = (default_timer() - self.batch_time)\n",
    "            self.batch_count += 1\n",
    "        except Exception as e:\n",
    "            self.f.close()\n",
    "            raise e\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        try:\n",
    "            self.f['.']['time']['train']['end_time'][0] = default_timer()\n",
    "            self.f['.']['config'].attrs[\"total_minibatches\"] = self.batch_count\n",
    "            self.f['.']['time_markers'].attrs['minibatches_complete'] = self.batch_count\n",
    "            self.f.close()\n",
    "        except Exception as e:\n",
    "            self.f.close()\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to dynamically change keras backend\n",
    "from importlib import reload\n",
    "def set_keras_backend(backend):\n",
    "    if K.backend() != backend:\n",
    "        os.environ['KERAS_BACKEND'] = backend\n",
    "        reload(K)\n",
    "        assert K.backend() == backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cntk_conv1 (Conv2D)          (None, 23, 23, 64)        4864      \n",
      "_________________________________________________________________\n",
      "cntk_pool1 (MaxPooling2D)    (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "cntk_conv2 (Conv2D)          (None, 11, 11, 256)       147712    \n",
      "_________________________________________________________________\n",
      "cntk_pool2 (MaxPooling2D)    (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "cntk_global_pool (GlobalAver (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "cntk_drop_out (Dropout)      (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "cntk_fc2 (Dense)             (None, 43)                11051     \n",
      "=================================================================\n",
      "Total params: 163,627\n",
      "Trainable params: 163,627\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 31367 samples, validate on 7842 samples\n",
      "Epoch 1/60\n",
      "  256/31367 [..............................] - ETA: 24s - loss: 15.5533 - acc: 0.0352"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhongyilin/miniconda3/envs/neon/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input697\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31232/31367 [============================>.] - ETA: 0s - loss: 15.2924 - acc: 0.0511Epoch 00000: val_loss improved from inf to 15.30829, saving model to /home/zhongyilin/Desktop/GTSRB/try/saved_models/keras_cntk_weights.hdf5\n",
      "31367/31367 [==============================] - 7s - loss: 15.2919 - acc: 0.0511 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 2/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2587 - acc: 0.0533Epoch 00001: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2589 - acc: 0.0533 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 3/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2581 - acc: 0.0533Epoch 00002: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2578 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 4/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2704 - acc: 0.0526Epoch 00003: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2701 - acc: 0.0526 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 5/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2598 - acc: 0.0533Epoch 00004: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2600 - acc: 0.0532 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 6/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2608 - acc: 0.0532Epoch 00005: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2610 - acc: 0.0532 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 7/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2571 - acc: 0.0534Epoch 00006: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2572 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 8/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2593 - acc: 0.0532Epoch 00007: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2590 - acc: 0.0532 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 9/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2577 - acc: 0.0534Epoch 00008: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2579 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 10/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2572 - acc: 0.0534Epoch 00009: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 11/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2572 - acc: 0.0534Epoch 00010: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2574 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 12/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2572 - acc: 0.0534Epoch 00011: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2574 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 13/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2570 - acc: 0.0534Epoch 00012: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2572 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 14/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2572 - acc: 0.0534Epoch 00013: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 15/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2567 - acc: 0.0534Epoch 00014: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 16/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2567 - acc: 0.0534Epoch 00015: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 17/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2577 - acc: 0.0534Epoch 00016: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 18/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2572 - acc: 0.0534Epoch 00017: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2574 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 19/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2567 - acc: 0.0534Epoch 00018: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 20/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2572 - acc: 0.0534Epoch 00019: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 21/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2567 - acc: 0.0534Epoch 00020: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 22/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2567 - acc: 0.0534Epoch 00021: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 23/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2572 - acc: 0.0534Epoch 00022: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 24/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2562 - acc: 0.0535Epoch 00023: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2564 - acc: 0.0535 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 25/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2572 - acc: 0.0534Epoch 00024: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 26/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2572 - acc: 0.0534Epoch 00025: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 27/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2572 - acc: 0.0534Epoch 00026: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 28/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2572 - acc: 0.0534Epoch 00027: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 29/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2567 - acc: 0.0534Epoch 00028: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 30/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2567 - acc: 0.0534Epoch 00029: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 31/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2567 - acc: 0.0534Epoch 00030: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 32/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2567 - acc: 0.0534Epoch 00031: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 33/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2577 - acc: 0.0534Epoch 00032: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2574 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 34/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2572 - acc: 0.0534Epoch 00033: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 35/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2567 - acc: 0.0534Epoch 00034: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 36/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2572 - acc: 0.0534Epoch 00035: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 37/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2572 - acc: 0.0534Epoch 00036: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 38/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2572 - acc: 0.0534Epoch 00037: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 39/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2567 - acc: 0.0534Epoch 00038: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 40/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2567 - acc: 0.0534Epoch 00039: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 41/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2572 - acc: 0.0534Epoch 00040: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 42/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2572 - acc: 0.0534Epoch 00041: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 43/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2577 - acc: 0.0534Epoch 00042: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 44/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2567 - acc: 0.0534Epoch 00043: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 45/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2567 - acc: 0.0534Epoch 00044: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 46/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2572 - acc: 0.0534Epoch 00045: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 47/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2567 - acc: 0.0534Epoch 00046: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 48/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2572 - acc: 0.0534Epoch 00047: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 49/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2572 - acc: 0.0534Epoch 00048: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 50/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2567 - acc: 0.0534Epoch 00049: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 51/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2567 - acc: 0.0534Epoch 00050: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 52/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2572 - acc: 0.0534Epoch 00051: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 53/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2567 - acc: 0.0534Epoch 00052: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 54/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2567 - acc: 0.0534Epoch 00053: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 55/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2567 - acc: 0.0534Epoch 00054: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 56/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2572 - acc: 0.0534Epoch 00055: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 57/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2567 - acc: 0.0534Epoch 00056: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 58/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2567 - acc: 0.0534Epoch 00057: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 59/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2567 - acc: 0.0534Epoch 00058: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "Epoch 60/60\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 15.2567 - acc: 0.0534Epoch 00059: val_loss did not improve\n",
      "31367/31367 [==============================] - 7s - loss: 15.2569 - acc: 0.0534 - val_loss: 15.3083 - val_acc: 0.0502\n",
      "cntk training finishes in 449.64 seconds.\n",
      "cntk test accuracy: 5.5%\n"
     ]
    }
   ],
   "source": [
    "from sys import platform\n",
    "backends = [\"tensorflow\", \"theano\"]\n",
    "if platform != \"darwin\":\n",
    "    backends.append(\"cntk\")\n",
    "\n",
    "for b in backends:\n",
    "    set_keras_backend(b)\n",
    "    \n",
    "    max_total_batch = (len(x_train) / batch_size + 1) * epoch_num\n",
    "    \n",
    "    # Load and process images\n",
    "    enc = OneHotEncoder(sparse=False)\n",
    "    keras_train_x = np.vstack([np.expand_dims(image.img_to_array(x), axis=0).astype('float32')/255 for x in x_train])\n",
    "    keras_valid_x = np.vstack([np.expand_dims(image.img_to_array(x), axis=0).astype('float32')/255 for x in x_valid])\n",
    "    keras_test_x = np.vstack([np.expand_dims(image.img_to_array(x), axis=0).astype('float32')/255 for x in testImages])\n",
    "    keras_train_y = to_categorical(y_train, 43)\n",
    "    keras_valid_y = to_categorical(y_valid, 43)\n",
    "    keras_test_y = to_categorical(testLabels, 43)\n",
    "\n",
    "    # Build model\n",
    "    keras_gaussInit = RandomNormal(mean=0.0, stddev=0.01, seed=542)\n",
    "    layer_name_prefix = b+\"_\"\n",
    "\n",
    "    keras_model = Sequential()\n",
    "    keras_model.add(keras_Conv(64, (5, 5), kernel_initializer=keras_gaussInit, strides=(2, 2), bias_initializer=keras_Constant(0.0), activation=\"relu\", input_shape=(resize_size[0], resize_size[1], 3), name=layer_name_prefix+\"conv1\"))\n",
    "    keras_model.add(keras_MaxPooling(pool_size=(2, 2), name=layer_name_prefix+\"pool1\"))\n",
    "    keras_model.add(keras_Conv(256, (3, 3), kernel_initializer=keras_gaussInit, strides=(1, 1), padding=\"same\", bias_initializer=keras_Constant(0.0), activation=\"relu\", name=layer_name_prefix+\"conv2\"))\n",
    "    keras_model.add(keras_MaxPooling(pool_size=(2, 2), name=layer_name_prefix+\"pool2\"))\n",
    "    keras_model.add(keras_AveragePooling(name=layer_name_prefix+\"global_pool\"))\n",
    "#     keras_model.add(Flatten(name=layer_name_prefix+\"flatten\")) # An extra layer to flatten the previous layer in order to connect to fully connected layer\n",
    "#     keras_model.add(Dense(4096, kernel_initializer=keras_gaussInit, bias_initializer=keras_Constant(0.0), activation=\"relu\", name=layer_name_prefix+\"fc1\"))\n",
    "    keras_model.add(keras_Dropout(0.5, name=layer_name_prefix+\"drop_out\"))\n",
    "    keras_model.add(Dense(43, kernel_initializer=keras_gaussInit, bias_initializer=keras_Constant(0.0), activation=\"softmax\", name=layer_name_prefix+\"fc2\"))\n",
    "    keras_model.summary()\n",
    "\n",
    "    keras_optimizer = keras_SGD(lr=0.01, decay=1.6e-8, momentum=0.9) # Equivalent to decay rate 0.2 per epoch? Need to re-verify\n",
    "#     keras_optimizer = keras_RMSProp(lr=0.01, decay=0.95)\n",
    "    keras_cost = \"categorical_crossentropy\"\n",
    "    keras_model.compile(loss=keras_cost, optimizer=keras_optimizer, metrics=[\"acc\"])\n",
    "\n",
    "    checkpointer = ModelCheckpoint(filepath=root+\"/saved_models/keras_\"+b+\"_weights.hdf5\",\n",
    "                                       verbose=1, save_best_only=True)\n",
    "    losses = LossHistory(root+\"/callback_data_{}.h5\".format(b), epoch_num, max_total_batch)\n",
    "\n",
    "    start = time.time()\n",
    "    keras_model.fit(keras_train_x, keras_train_y,\n",
    "                  validation_data=(keras_valid_x, keras_valid_y),\n",
    "                  epochs=epoch_num, batch_size=batch_size, callbacks=[checkpointer, losses], verbose=1, shuffle=True)\n",
    "    print(\"{} training finishes in {:.2f} seconds.\".format(b, time.time() - start))\n",
    "\n",
    "    keras_model.load_weights(root+\"/saved_models/keras_\"+b+\"_weights.hdf5\")\n",
    "    keras_predictions = [np.argmax(keras_model.predict(np.expand_dims(feature, axis=0))) for feature in keras_test_x]\n",
    "\n",
    "    # report test accuracy\n",
    "    keras_test_accuracy = 100*np.sum(np.array(keras_predictions)==np.argmax(keras_test_y, axis=1))/len(keras_predictions)\n",
    "    print('{} test accuracy: {:.1f}%'.format(b, keras_test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as utils\n",
    "import torch.nn.init as torch_init\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        # Build model\n",
    "        self.conv = torch.nn.Sequential()\n",
    "        self.conv.add_module(\"torch_conv1\", torch.nn.Conv2d(3, 64, kernel_size=(5, 5), stride=2))\n",
    "        self.conv.add_module(\"torch_pool1\", torch.nn.MaxPool2d(kernel_size=2))\n",
    "        self.conv.add_module(\"torch_relu1\", torch.nn.ReLU())\n",
    "        self.conv.add_module(\"torch_conv2\", torch.nn.Conv2d(64, 256, kernel_size=(3, 3), stride=1, padding=1))\n",
    "        self.conv.add_module(\"torch_pool2\", torch.nn.MaxPool2d(kernel_size=2))\n",
    "        self.conv.add_module(\"torch_relu2\", torch.nn.ReLU())\n",
    "        self.conv.add_module(\"torch_global_pool\", torch.nn.AvgPool2d(kernel_size=5))\n",
    "        \n",
    "        self.csf = torch.nn.Sequential()\n",
    "        self.csf.add_module(\"torch_fc1\", torch.nn.Linear(256, 4096))\n",
    "        self.csf.add_module(\"torch_relu3\", torch.nn.ReLU())\n",
    "        self.csf.add_module(\"torch_dropout1\", torch.nn.Dropout(0.5))\n",
    "        self.csf.add_module(\"torch_fc2\", torch.nn.Linear(4096, 43))\n",
    "        \n",
    "        # Initialize conv layers and fc layers\n",
    "        torch_init.normal(self.conv.state_dict()[\"torch_conv1.weight\"], mean=0, std=0.01)\n",
    "        torch_init.constant(self.conv.state_dict()[\"torch_conv1.bias\"], 0.0)\n",
    "        torch_init.normal(self.conv.state_dict()[\"torch_conv2.weight\"], mean=0, std=0.01)\n",
    "        torch_init.constant(self.conv.state_dict()[\"torch_conv2.bias\"], 0.0)\n",
    "        torch_init.normal(self.csf.state_dict()[\"torch_fc1.weight\"], mean=0, std=0.01)\n",
    "        torch_init.constant(self.csf.state_dict()[\"torch_fc1.bias\"], 0.0)\n",
    "        torch_init.normal(self.csf.state_dict()[\"torch_fc2.weight\"], mean=0, std=0.01)\n",
    "        torch_init.constant(self.csf.state_dict()[\"torch_fc2.bias\"], 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv.forward(x)\n",
    "        x = x.view(-1, 256)\n",
    "        return self.csf.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch_train_x = torch.stack([torch.Tensor(i.swapaxes(0,2).astype(\"float32\")/255) for i in x_train])\n",
    "torch_train_y = torch.LongTensor(y_train)\n",
    "torch_valid_x = torch.stack([torch.Tensor(i.swapaxes(0,2).astype(\"float32\")/255) for i in x_valid])\n",
    "torch_valid_y = torch.LongTensor(y_valid)\n",
    "torch_test_x = torch.stack([torch.Tensor(i.swapaxes(0,2).astype(\"float32\")/255) for i in testImages])\n",
    "torch_test_y = torch.LongTensor(testLabels)\n",
    "\n",
    "torch_tensor_train_set = utils.TensorDataset(torch_train_x, torch_train_y)\n",
    "torch_train_set = utils.DataLoader(torch_tensor_train_set, batch_size=batch_size, shuffle=True)\n",
    "torch_tensor_valid_set = utils.TensorDataset(torch_valid_x, torch_valid_y)\n",
    "torch_valid_set = utils.DataLoader(torch_tensor_valid_set, batch_size=batch_size, shuffle=True)\n",
    "torch_tensor_test_set = utils.TensorDataset(torch_test_x, torch_test_y)\n",
    "torch_test_set = utils.DataLoader(torch_tensor_test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "torch_model = ConvNet()\n",
    "optimizer = optim.SGD(torch_model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    torch_model.train()\n",
    "    for batch_idx, (data, target) in enumerate(torch_train_set):\n",
    "#         if args.cuda:\n",
    "#             data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = torch_model(data)\n",
    "        cost = torch.nn.CrossEntropyLoss(size_average=True)\n",
    "        loss = cost(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(torch_train_set.dataset),\n",
    "                100. * batch_idx / len(torch_train_set), loss.data[0]))\n",
    "def test():\n",
    "    torch_model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in torch_test_set:\n",
    "#         if args.cuda:\n",
    "#             data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = torch_model(data)\n",
    "        cost = torch.nn.CrossEntropyLoss(size_average=False)\n",
    "        test_loss += cost(output, target).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(torch_test_set.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(torch_test_set.dataset),\n",
    "        100. * correct / len(torch_test_set.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, epoch_num + 1):\n",
    "    train(epoch)\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.DEBUG)  # logging to stdout\n",
    "\n",
    "epoch_num = 5\n",
    "batch_size = 128\n",
    "resize_size = (49, 49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mx_train_x = mx.nd.array([i.swapaxes(0,2).astype(\"float32\")/255 for i in x_train])\n",
    "mx_valid_x = mx.nd.array([i.swapaxes(0,2).astype(\"float32\")/255 for i in x_valid])\n",
    "mx_test_x = mx.nd.array([i.swapaxes(0,2).astype(\"float32\")/255 for i in testImages])\n",
    "mx_train_y = mx.nd.array(y_train, dtype=np.float32) # No need of one_hot\n",
    "mx_valid_y = mx.nd.array(y_valid, dtype=np.float32)\n",
    "mx_test_y = mx.nd.array(testLabels, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The iterators have input name of 'data' and output name of 'softmax_label' if not particularly specified\n",
    "mx_train_set = mx.io.NDArrayIter(mx_train_x, mx_train_y, batch_size, shuffle=True)\n",
    "mx_valid_set = mx.io.NDArrayIter(mx_valid_x, mx_valid_y, batch_size)\n",
    "mx_test_set = mx.io.NDArrayIter(mx_test_x, mx_test_y, batch_size)\n",
    "\n",
    "# Print the shape and type of training set lapel\n",
    "# mx_train_set.provide_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = mx.sym.Variable('data')\n",
    "mx_conv1 = mx.sym.Convolution(data = data, name='mx_conv1', num_filter=64, kernel=(5,5), stride=(2,2))\n",
    "mx_act1 = mx.sym.Activation(data = mx_conv1, name='mx_relu1', act_type=\"relu\")\n",
    "mx_mp1 = mx.sym.Pooling(data = mx_act1, name = 'mx_pool1', kernel=(2,2), stride=(2,2), pool_type='max')\n",
    "mx_conv2 = mx.sym.Convolution(data = mx_mp1, name='mx_conv2', num_filter=512, kernel=(3,3), stride=(1,1), pad=(1,1))\n",
    "mx_act2 = mx.sym.Activation(data = mx_conv2, name='mx_relu2', act_type=\"relu\")\n",
    "mx_mp2 = mx.sym.Pooling(data = mx_act2, name = 'mx_pool2', kernel=(2,2), stride=(2,2), pool_type='max')\n",
    "mx_fl = mx.sym.Flatten(data = mx_mp2, name=\"mx_flatten\")\n",
    "mx_fc1 = mx.sym.FullyConnected(data = mx_fl, name='mx_fc1', num_hidden=2048)\n",
    "mx_drop = mx.sym.Dropout(data = mx_fc1, name='mx_dropout', p=0.5)\n",
    "mx_fc2 = mx.sym.FullyConnected(data = mx_drop, name='mx_fc2', num_hidden=43)\n",
    "mx_softmax = mx.sym.SoftmaxOutput(data = mx_fc2, name ='softmax')\n",
    "\n",
    "# Print the names of arguments in the model\n",
    "# mx_softmax.list_arguments() # Make sure the input and the output names are consistent of those in the iterator!!\n",
    "\n",
    "# Print the size of the model\n",
    "# mx_softmax.infer_shape(data=(1,3,49,49))\n",
    "\n",
    "# Draw the network\n",
    "# mx.viz.plot_network(mx_softmax, shape={\"data\":(batch_size, 3, resize_size[0], resize_size[1])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MxCustomInit(mx.initializer.Initializer):\n",
    "    def __init__(self, idict):\n",
    "        super(MxCustomInit, self).__init__()\n",
    "        self.dict = idict\n",
    "        np.random.seed(seed=1)\n",
    "\n",
    "    def _init_weight(self, name, arr):\n",
    "        if name in self.dict.keys():\n",
    "            dictPara = self.dict[name]\n",
    "            for(k, v) in dictPara.items():\n",
    "                arr = np.random.normal(0, v, size=arr.shape)\n",
    "\n",
    "    def _init_bias(self, name, arr):\n",
    "        if name in self.dict.keys():\n",
    "            dictPara = self.dict[name]\n",
    "            for(k, v) in dictPara.items():\n",
    "                arr[:] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mx_nor_dict = {'normal': 0.01}\n",
    "mx_cons_dict = {'constant': 0.0}\n",
    "mx_init_dict = {}\n",
    "for layer in mx_softmax.list_arguments():\n",
    "    hh = layer.split('_')\n",
    "    if hh[-1] == 'weight':\n",
    "        mx_init_dict[layer] = mx_nor_dict\n",
    "    elif hh[-1] == 'bias':\n",
    "        mx_init_dict[layer] = mx_cons_dict\n",
    "# print(mx_init_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a trainable module on CPU\n",
    "mx_model = mx.mod.Module(context = mx.cpu(), symbol = mx_softmax)\n",
    "\n",
    "# Currently no solution to reproducibility. Eyes on issue 47.\n",
    "mx_model.fit(mx_train_set, # train data\n",
    "             eval_data = mx_valid_set, # validation data\n",
    "             num_epoch = epoch_num,\n",
    "             initializer = MxCustomInit(mx_init_dict),\n",
    "             optimizer = 'sgd',\n",
    "             optimizer_params = {'learning_rate': 0.1, 'momentum': 0.9},\n",
    "             eval_metric ='acc', # report accuracy during training\n",
    "             batch_end_callback = mx.callback.Speedometer(batch_size, 10)) # output progress for each 10 data batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = mx_model.score(mx_test_set, ['acc'])\n",
    "print(\"Accuracy score is %f\" % (score[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
